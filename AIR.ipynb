{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MwEr46hqDtJK",
    "outputId": "e800c63d-a5a3-4381-ca15-0386e923180e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wget in /home/niklas/.local/lib/python3.10/site-packages (3.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/niklas/.local/lib/python3.10/site-packages (4.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/niklas/.local/lib/python3.10/site-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/niklas/.local/lib/python3.10/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/niklas/.local/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/niklas/.local/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/niklas/.local/lib/python3.10/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/niklas/.local/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/niklas/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ray in /home/niklas/.local/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: jsonschema in /home/niklas/.local/lib/python3.10/site-packages (from ray) (4.17.0)\n",
      "Requirement already satisfied: grpcio>=1.42.0 in /home/niklas/.local/lib/python3.10/site-packages (from ray) (1.51.1)\n",
      "Requirement already satisfied: click>=7.0 in /usr/lib/python3/dist-packages (from ray) (8.0.3)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /home/niklas/.local/lib/python3.10/site-packages (from ray) (4.21.12)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /home/niklas/.local/lib/python3.10/site-packages (from ray) (1.23.4)\n",
      "Requirement already satisfied: frozenlist in /home/niklas/.local/lib/python3.10/site-packages (from ray) (1.3.3)\n",
      "Requirement already satisfied: packaging in /home/niklas/.local/lib/python3.10/site-packages (from ray) (21.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray) (3.8.0)\n",
      "Requirement already satisfied: attrs in /home/niklas/.local/lib/python3.10/site-packages (from ray) (22.1.0)\n",
      "Requirement already satisfied: aiosignal in /home/niklas/.local/lib/python3.10/site-packages (from ray) (1.3.1)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from ray) (5.4.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray) (2.28.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/niklas/.local/lib/python3.10/site-packages (from ray) (1.0.4)\n",
      "Requirement already satisfied: virtualenv>=20.0.24 in /usr/local/lib/python3.10/dist-packages (from ray) (20.16.7)\n",
      "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.0.24->ray) (0.3.6)\n",
      "Requirement already satisfied: platformdirs<3,>=2.4 in /home/niklas/.local/lib/python3.10/site-packages (from virtualenv>=20.0.24->ray) (2.5.4)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/niklas/.local/lib/python3.10/site-packages (from jsonschema->ray) (0.19.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging->ray) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install wget\n",
    "!pip install transformers\n",
    "!pip install ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "N1mZmWzU18_W"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import wget\n",
    "import gzip\n",
    "import html\n",
    "import tarfile\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, AdamW, BertTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mCd6h66j7mKf"
   },
   "outputs": [],
   "source": [
    "class ProductRanker(nn.Module):\n",
    "    def __init__(self, l1=32, l2=256):\n",
    "        super(ProductRanker, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.ranker = nn.Sequential(\n",
    "            nn.Linear(768, l1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l1, l2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l2,1)\n",
    "        )\n",
    "\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        logits = self.ranker(last_hidden_state_cls)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tA9ILTLDDmkE",
    "outputId": "dc6fdd1a-a304-4954-fa55-098d481ee549"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niklas/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:88: UserWarning: HIP initialization: Unexpected error from hipGetDeviceCount(). Did you run some cuda functions before calling NumHipDevices() that might have already set an error? Error 101: hipErrorInvalidDevice (Triggered internally at ../c10/hip/HIPFunctions.cpp:110.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#Constants\n",
    "config = {'l1': 256, 'l2': 8, 'lr': 0.05557941010651975, 'batch_size': 4}\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ProductRanker(config[\"l1\"],config[\"l2\"]).to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], betas=(0.9, 0.999), eps=1e-08, weight_decay=0,\n",
    "                            amsgrad=False)\n",
    "\n",
    "if not os.path.exists(\"./data\"):\n",
    "    os.makedirs(\"./data\")\n",
    "if not os.path.exists(\"./outputs\"):\n",
    "    os.makedirs(\"./outputs\")\n",
    "\n",
    "logging.basicConfig(filename='./outputs/training.log', filemode='w', format='%(asctime)s %(message)s', level=logging.DEBUG, force=True)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "g35D5YJf3Ila"
   },
   "outputs": [],
   "source": [
    "def downloadData(url, path):\n",
    "    return wget.download(url, out=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OY40JDngA7sB"
   },
   "outputs": [],
   "source": [
    "def downloadData(url, path):\n",
    "    return wget.download(url, out=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "V4-NKG6m3OjZ"
   },
   "outputs": [],
   "source": [
    "def loadJson(filename):\n",
    "    data = []\n",
    "    with gzip.open(filename, \"rt\") as f:\n",
    "        for line in f:\n",
    "            stripped = line.strip()\n",
    "            stripped = stripped.replace(\"\\'\", \"\\\"\")\n",
    "            try:\n",
    "                data.append(json.loads(stripped))\n",
    "            except json.decoder.JSONDecodeError:\n",
    "                continue \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "CZatCtPg3QRz"
   },
   "outputs": [],
   "source": [
    "def filterJson(data, extrema_dict):\n",
    "    filtered = []\n",
    "    for object in data:\n",
    "        if not isGoodJsonEntry(object):\n",
    "            continue\n",
    "        \n",
    "        sales_rank_key = list(object['salesRank'].keys())[0]\n",
    "        sales_rank_value = object['salesRank'][sales_rank_key]\n",
    "        min = extrema_dict[sales_rank_key][0]\n",
    "        max = extrema_dict[sales_rank_key][1]\n",
    "\n",
    "        tmp = {}\n",
    "        tmp['description'] = html.unescape(object['description'])\n",
    "        tmp['salesRank'] = (sales_rank_value - min) / (max - min)\n",
    "        filtered.append(tmp)\n",
    "\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uFHxm4vW3TU6"
   },
   "outputs": [],
   "source": [
    "def getExtremaDict(data):\n",
    "    extrema_dict = {}\n",
    "    for object in data:\n",
    "        if not isGoodJsonEntry(object):\n",
    "            continue\n",
    "\n",
    "        sales_rank_key = list(object['salesRank'].keys())[0]\n",
    "        sales_rank_value = object['salesRank'][sales_rank_key]\n",
    "        if sales_rank_key not in extrema_dict:\n",
    "            extrema_dict[sales_rank_key] = [sales_rank_value, sales_rank_value] \n",
    "        else:\n",
    "            if extrema_dict[sales_rank_key][0] > sales_rank_value:\n",
    "                extrema_dict[sales_rank_key][0] = sales_rank_value\n",
    "            elif extrema_dict[sales_rank_key][1] < sales_rank_value:\n",
    "                extrema_dict[sales_rank_key][1] = sales_rank_value\n",
    "\n",
    "    return extrema_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lUCHiFQx3Vjo"
   },
   "outputs": [],
   "source": [
    "def isGoodJsonEntry(object):\n",
    "    if 'description' not in object.keys():\n",
    "        return False\n",
    "    if len(object['description']) == 0:\n",
    "        return False\n",
    "    if 'salesRank' not in object.keys():\n",
    "        return False\n",
    "    if len(object['salesRank']) == 0:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "iYBaprFg3YFW"
   },
   "outputs": [],
   "source": [
    "def safeJson(filtered, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for object in filtered:\n",
    "            json.dump(object, f)\n",
    "            f.write(os.linesep)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Qxt6iaSp3Zmv"
   },
   "outputs": [],
   "source": [
    "def getJSON(data_dir=\"./data\"):\n",
    "    filename = os.path.join(data_dir, \"meta_Electronics.json.gz\")\n",
    "    filename_filtered = os.path.join(data_dir, \"filtered_meta_Electronics.tar.gz\")\n",
    "    filename_destination = os.path.join(data_dir, \"filtered_meta_Electronics.json\")\n",
    "\n",
    "    if os.path.exists(filename_filtered):\n",
    "        file = tarfile.open(filename_filtered, \"r:gz\")\n",
    "        file.extractall(data_dir)\n",
    "        file.close()\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        url = \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/meta_Electronics.json.gz\"\n",
    "        downloadData(url, filename)\n",
    "    \n",
    "    data = loadJson(filename)\n",
    "    extrema_dict = getExtremaDict(data)\n",
    "    filtered = filterJson(data, extrema_dict)\n",
    "    safeJson(filtered, filename_destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nm7U4hna8G7j"
   },
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "eMavKxtY8Bv6"
   },
   "outputs": [],
   "source": [
    "def dataPreparation(data_dir=\"./data\"):\n",
    "    path = os.path.join(data_dir, \"filtered_meta_Electronics.json\")\n",
    "    print(path)\n",
    "    df = pd.read_json(path, lines=True).sample(n = 10)\n",
    "    #df = pd.read_json(\"filtered_meta_Electronics.json\", lines=True)[:110]\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    tokenized_set = []\n",
    "    for index, row in df.iterrows():\n",
    "        tokenized_set.append([tokenizer.encode_plus(row['description'], truncation = True, return_tensors=\"pt\",\n",
    "                                                    max_length=512, pad_to_max_length=True), row['salesRank']])\n",
    "    return torch.utils.data.random_split(tokenized_set, [8, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_-y6YFzn8SWW"
   },
   "outputs": [],
   "source": [
    "def createDataloader(dataset, config, shuffle=True):\n",
    "    labeled_set = []\n",
    "    for i, doc1 in enumerate(dataset):\n",
    "        for j, doc2 in enumerate(dataset):\n",
    "            if i != j:\n",
    "                label = 0.0\n",
    "                if doc1[1] > doc2[1]:\n",
    "                    label = 1.0\n",
    "                if doc1[1] == doc2[1]:\n",
    "                    label = 0.5\n",
    "\n",
    "                labeled_set.append([[doc1[0], doc2[0]], label])\n",
    "    return torch.utils.data.DataLoader(labeled_set, batch_size=int(config[\"batch_size\"]), num_workers=2, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "f7k5M2SU8Zw8"
   },
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, dataloader, epoch, saving=False, tune=False):\n",
    "    running_loss = 0.0\n",
    "    epoch_steps = 0\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for X,y in dataloader:\n",
    "        input_ids_1 = X[0]['input_ids'].squeeze().to(device)\n",
    "        attention_mask_1 = X[0]['attention_mask'].squeeze().to(device)\n",
    "        input_ids_2 = X[1]['input_ids'].squeeze().to(device)\n",
    "        attention_mask_2 = X[1]['attention_mask'].squeeze().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out1 = model.forward(input_ids_1, attention_mask_1)\n",
    "        out2 = model.forward(input_ids_2, attention_mask_2)\n",
    "        diff = (out1 - out2).squeeze()\n",
    "        diff = torch.sigmoid(diff).to(device)\n",
    "        loss = loss_fn(diff, y.float().to(device))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    logger.info(\"[%d] train loss: %.10f\" % (epoch + 1, train_loss / len(dataloader)))\n",
    "\n",
    "    if saving:\n",
    "        path = './outputs/model_' + str(epoch+1) + '.pth'\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }, path)\n",
    "        print(\"Model saved..\")\n",
    "\n",
    "    if tune:\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "    print(\"Training finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "V4-vkMnZ8cq7"
   },
   "outputs": [],
   "source": [
    "def val(model, loss_fn, dataloader, epoch, tune=False):\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        for X, y in dataloader:\n",
    "            with torch.no_grad():\n",
    "                input_ids_1 = X[0]['input_ids'].squeeze().to(device)\n",
    "                attention_mask_1 = X[0]['attention_mask'].squeeze().to(device)\n",
    "                input_ids_2 = X[1]['input_ids'].squeeze().to(device)\n",
    "                attention_mask_2 = X[1]['attention_mask'].squeeze().to(device)\n",
    "                out1 = model.forward(input_ids_1, attention_mask_1)\n",
    "                out2 = model.forward(input_ids_2, attention_mask_2)\n",
    "                diff = (out1 - out2).squeeze()\n",
    "                diff = torch.sigmoid(diff)\n",
    "                loss = loss_fn(diff, y.float().to(device))\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        if(tune):\n",
    "            tune.report(loss=val_loss)\n",
    "\n",
    "        logger.info(\"[%d] val loss: %.10f\" % (epoch+1, val_loss))\n",
    "        return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "PEGxtzDEPLDQ"
   },
   "outputs": [],
   "source": [
    "def train_tune(config, data_dir=\"./data\"):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = ProductRanker(config[\"l1\"],config[\"l2\"]).to(device)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], betas=(0.9, 0.999), eps=1e-08, weight_decay=0,\n",
    "                                amsgrad=False)\n",
    "    \n",
    "    train_set, val_set = dataPreparation(data_dir)\n",
    "    dataloader_train = createDataloader(train_set, config)\n",
    "    dataloader_val = createDataloader(val_set, config)\n",
    "\n",
    "    train(model, optimizer, dataloader_train, 0, tune=True)\n",
    "    val(model, loss_fn, dataloader_val, 0, tune=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xLrYjbv9QJx"
   },
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3mrEAanp9OFO"
   },
   "outputs": [],
   "source": [
    "def tune_hyperparameters():\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    config = {\n",
    "        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([2, 4, 8, 16])\n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=4,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    reporter = CLIReporter(\n",
    "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        partial(train_tune, data_dir=data_dir),\n",
    "        resources_per_trial={\"cpu\": 12, \"gpu\": 1},\n",
    "        config=config,\n",
    "        num_samples=10,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "YStntQVRDUGM"
   },
   "outputs": [],
   "source": [
    "def tune_lr():\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    config = {\n",
    "        'l1': 256,\n",
    "        'l2': 8,\n",
    "        'lr': tune.loguniform(1e-4, 1e-1),\n",
    "        'batch_size': 4}\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=5,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    reporter = CLIReporter(\n",
    "        metric_columns=[\"loss\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        partial(train_tune, data_dir=data_dir),\n",
    "        resources_per_trial={\"cpu\": 12, \"gpu\": 1},\n",
    "        local_dir=\"./ray_results\",\n",
    "        config=config,\n",
    "        num_samples=5,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    best_checkpoint = result.get_best_checkpoint(best_trial, metric=\"loss\", mode=\"min\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "NmPbsO6w9c7T"
   },
   "outputs": [],
   "source": [
    "def train_model(epochs, config, from_checkpoint=False, path=None):\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    train(epochs, config, data_dir, from_checkpoint, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "rQ6Va6c59fP1"
   },
   "outputs": [],
   "source": [
    "def main(epochs, train_bool=True, validate_bool=True, data_dir=\"./data\", saving=False, load=False):\n",
    "    getJSON()\n",
    "    train_set, val_set = dataPreparation()\n",
    "    dataloader_train = createDataloader(train_set, config)\n",
    "    dataloader_val = createDataloader(val_set, config)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if load:\n",
    "            current_model_filename = './outputs/model_' + str(epoch+1) + '.pth'\n",
    "            if os.path.exists(current_model_filename):\n",
    "                    checkpoint = torch.load(current_model_filename)\n",
    "                    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            else:\n",
    "                logger.info(f\"model: {current_model_filename} doesnt exist\")\n",
    "        \n",
    "        logger.info(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "        if train_bool:\n",
    "            logger.info(\"Starting training loop...\")\n",
    "            train(model, loss_fn, optimizer, dataloader_train, epoch, saving=saving, tune=False)\n",
    "        \n",
    "        if validate_bool:\n",
    "            logger.info(\"Starting validation loop...\")\n",
    "            val(model, loss_fn, dataloader_val, epoch, tune=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C7vVclae-Ze6",
    "outputId": "747cb3ef-f2b6-4e92-9fa6-d8fa362eacfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "100% [..................................................] 186594679 / 186594679./data/filtered_meta_Electronics.json\n",
      "Starting new HTTPS connection (1): huggingface.co:443\n",
      "https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/vocab.txt HTTP/1.1\" 200 0\n",
      "model: ./outputs/model_1.pth doesnt exist\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Starting training loop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niklas/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "main(5, saving=True, load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GBxiaLnjXIfV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
