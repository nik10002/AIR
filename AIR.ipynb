{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MwEr46hqDtJK",
    "outputId": "e800c63d-a5a3-4381-ca15-0386e923180e"
   },
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "!pip install transformers\n",
    "!pip install ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1mZmWzU18_W"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import wget\n",
    "import gzip\n",
    "import html\n",
    "import tarfile\n",
    "import logging\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, AdamW, BertTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCd6h66j7mKf"
   },
   "outputs": [],
   "source": [
    "class ProductRanker(nn.Module):\n",
    "    def __init__(self, l1=32, l2=256):\n",
    "        super(ProductRanker, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.ranker = nn.Sequential(\n",
    "            nn.Linear(768, l1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l1, l2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l2,1)\n",
    "        )\n",
    "\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        logits = self.ranker(last_hidden_state_cls)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tA9ILTLDDmkE",
    "outputId": "dc6fdd1a-a304-4954-fa55-098d481ee549"
   },
   "outputs": [],
   "source": [
    "#Constants\n",
    "config = {'l1': 256, 'l2': 8, 'lr': 0.0008611640946726886, 'batch_size': 4}\n",
    "model = ProductRanker(config[\"l1\"],config[\"l2\"]).to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], betas=(0.9, 0.999), eps=1e-08, weight_decay=0,\n",
    "                            amsgrad=False)\n",
    "\n",
    "if not os.path.exists(\"./data\"):\n",
    "    os.makedirs(\"./data\")\n",
    "if not os.path.exists(\"./outputs\"):\n",
    "    os.makedirs(\"./outputs\")\n",
    "\n",
    "logging.basicConfig(filename='./outputs/training.log', filemode='w', format='%(asctime)s %(message)s', level=logging.DEBUG, force=True)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g35D5YJf3Ila"
   },
   "outputs": [],
   "source": [
    "def downloadData(url, path):\n",
    "    return wget.download(url, out=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadRatings(filename, threshold=5):\n",
    "    ratings = {}\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        \n",
    "        for line in reader:\n",
    "            key = line[1]\n",
    "            value = float(line[2])\n",
    "            \n",
    "            if key not in ratings:\n",
    "                ratings[key] = [value]\n",
    "            else:\n",
    "                ratings[key].append(value)\n",
    "\n",
    "    return {key: sum(value)/len(value) for key, value in ratings.items() if len(value) >= threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4-NKG6m3OjZ"
   },
   "outputs": [],
   "source": [
    "def loadJson(filename):\n",
    "    data = []\n",
    "    with gzip.open(filename, \"rt\") as f:\n",
    "        for line in f:\n",
    "            stripped = line.strip()\n",
    "            stripped = stripped.replace(\"\\'\", \"\\\"\")\n",
    "            try:\n",
    "                data.append(json.loads(stripped))\n",
    "            except json.decoder.JSONDecodeError:\n",
    "                continue \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZatCtPg3QRz"
   },
   "outputs": [],
   "source": [
    "def filterJson(data, extrema_dict, ratings):\n",
    "    filtered = []\n",
    "    for entry in data:\n",
    "        if not isGoodJsonEntry(entry, ratings):\n",
    "            continue\n",
    "        \n",
    "        sales_rank_key = list(entry['salesRank'].keys())[0]\n",
    "        sales_rank_value = entry['salesRank'][sales_rank_key]\n",
    "        min = extrema_dict[sales_rank_key][0]\n",
    "        max = extrema_dict[sales_rank_key][1]\n",
    "\n",
    "        tmp = {}\n",
    "        tmp['ratings'] = ratings[entry['asin']]\n",
    "        if min == max:\n",
    "            tmp['salesRank'] = 0.5\n",
    "        else:\n",
    "            tmp['salesRank'] = (sales_rank_value - min) / (max - min)\n",
    "        tmp['description'] = html.unescape(entry['description'])\n",
    "        filtered.append(tmp)\n",
    "\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFHxm4vW3TU6"
   },
   "outputs": [],
   "source": [
    "def getExtremaDict(data, ratings):\n",
    "    extrema_dict = {}\n",
    "    for entry in data:\n",
    "        if not isGoodJsonEntry(entry, ratings):\n",
    "            continue\n",
    "\n",
    "        sales_rank_key = list(entry['salesRank'].keys())[0]\n",
    "        sales_rank_value = entry['salesRank'][sales_rank_key]\n",
    "        if sales_rank_key not in extrema_dict:\n",
    "            extrema_dict[sales_rank_key] = [sales_rank_value, sales_rank_value] \n",
    "        else:\n",
    "            if extrema_dict[sales_rank_key][0] > sales_rank_value:\n",
    "                extrema_dict[sales_rank_key][0] = sales_rank_value\n",
    "            elif extrema_dict[sales_rank_key][1] < sales_rank_value:\n",
    "                extrema_dict[sales_rank_key][1] = sales_rank_value\n",
    "\n",
    "    return extrema_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUCHiFQx3Vjo"
   },
   "outputs": [],
   "source": [
    "def isGoodJsonEntry(entry,ratings):\n",
    "    if 'description' not in entry.keys():\n",
    "        return False\n",
    "    if len(entry['description']) == 0:\n",
    "        return False\n",
    "    if 'salesRank' not in entry.keys():\n",
    "        return False\n",
    "    if len(entry['salesRank']) == 0:\n",
    "        return False\n",
    "    if not entry['asin'] in ratings:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iYBaprFg3YFW"
   },
   "outputs": [],
   "source": [
    "def safeJson(filtered, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for object in filtered:\n",
    "            json.dump(object, f)\n",
    "            f.write(os.linesep)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qxt6iaSp3Zmv"
   },
   "outputs": [],
   "source": [
    "def getJSON(data_dir=\"./data\"):\n",
    "    filename_meta = os.path.join(data_dir, \"meta_Electronics.json.gz\")\n",
    "    filename_filtered_meta = os.path.join(data_dir, \"filtered_meta_Electronics.tar.gz\")\n",
    "    filename_destination_meta = os.path.join(data_dir, \"filtered_meta_Electronics.json\")\n",
    "    filename_ratings = os.path.join(data_dir, \"ratings_Electronics.csv\")\n",
    "\n",
    "    url_meta = \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/meta_Electronics.json.gz\"\n",
    "    url_reviews = \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Electronics.csv\"\n",
    "\n",
    "    if os.path.exists(filename_destination_meta):\n",
    "        return\n",
    "\n",
    "    if os.path.exists(filename_filtered_meta):\n",
    "        file = tarfile.open(filename_filtered_meta, \"r:gz\")\n",
    "        file.extractall(data_dir)\n",
    "        file.close()\n",
    "        return\n",
    "        \n",
    "    if not os.path.exists(filename_ratings):\n",
    "        downloadData(url_reviews, filename_meta)\n",
    "\n",
    "    if not os.path.exists(filename_meta):\n",
    "        downloadData(url_meta, filename_meta)\n",
    "    \n",
    "    ratings = loadRatings(filename_ratings)\n",
    "    data = loadJson(filename_meta)\n",
    "    extrema_dict = getExtremaDict(data, ratings)\n",
    "    filtered = filterJson(data, extrema_dict, ratings)\n",
    "    safeJson(filtered, filename_destination_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nm7U4hna8G7j"
   },
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMavKxtY8Bv6"
   },
   "outputs": [],
   "source": [
    "def dataPreparation(data_dir=\"./data\"):\n",
    "    path = os.path.join(data_dir, \"filtered_meta_Electronics.json\")\n",
    "    df = pd.read_json(path, lines=True).sample(n = 120)\n",
    "    #df = pd.read_json(\"filtered_meta_Electronics.json\", lines=True)[:110]\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    tokenized_set = []\n",
    "    for index, row in df.iterrows():\n",
    "        tokenized_set.append([tokenizer.encode_plus(row['description'], truncation = True, return_tensors=\"pt\",\n",
    "                                                    max_length=512, pad_to_max_length=True), row['salesRank']])\n",
    "    return torch.utils.data.random_split(tokenized_set, [100, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-y6YFzn8SWW"
   },
   "outputs": [],
   "source": [
    "def createDataloader(dataset, config, shuffle=True):\n",
    "    labeled_set = []\n",
    "    for i, doc1 in enumerate(dataset):\n",
    "        for j, doc2 in enumerate(dataset):\n",
    "            if i != j:\n",
    "                label = 0.0\n",
    "                if doc1[1] > doc2[1]:\n",
    "                    label = 1.0\n",
    "                if doc1[1] == doc2[1]:\n",
    "                    label = 0.5\n",
    "\n",
    "                labeled_set.append([[doc1[0], doc2[0]], label])\n",
    "    return torch.utils.data.DataLoader(labeled_set, batch_size=int(config[\"batch_size\"]), num_workers=2, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7k5M2SU8Zw8"
   },
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, dataloader, epoch, saving=False, tune=False):\n",
    "    running_loss = 0.0\n",
    "    epoch_steps = 0\n",
    "\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for X,y in dataloader:\n",
    "        input_ids_1 = X[0]['input_ids'].squeeze().to(device)\n",
    "        attention_mask_1 = X[0]['attention_mask'].squeeze().to(device)\n",
    "        input_ids_2 = X[1]['input_ids'].squeeze().to(device)\n",
    "        attention_mask_2 = X[1]['attention_mask'].squeeze().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out1 = model.forward(input_ids_1, attention_mask_1)\n",
    "        out2 = model.forward(input_ids_2, attention_mask_2)\n",
    "        diff = (out1 - out2).squeeze()\n",
    "        diff = torch.sigmoid(diff).to(device)\n",
    "        loss = loss_fn(diff, y.float().to(device))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    logger.info(\"[%d] train loss: %.10f\" % (epoch + 1, train_loss / len(dataloader)))\n",
    "\n",
    "    if saving:\n",
    "        path = './outputs/model_' + str(epoch+1) + '.pth'\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }, path)\n",
    "        logger.info(\"Model saved..\")\n",
    "\n",
    "    if tune:\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "    logger.info(\"Training finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4-vkMnZ8cq7"
   },
   "outputs": [],
   "source": [
    "def val(model, loss_fn, dataloader, epoch, tune=False):\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        for X, y in dataloader:\n",
    "            with torch.no_grad():\n",
    "                input_ids_1 = X[0]['input_ids'].squeeze().to(device)\n",
    "                attention_mask_1 = X[0]['attention_mask'].squeeze().to(device)\n",
    "                input_ids_2 = X[1]['input_ids'].squeeze().to(device)\n",
    "                attention_mask_2 = X[1]['attention_mask'].squeeze().to(device)\n",
    "                out1 = model.forward(input_ids_1, attention_mask_1)\n",
    "                out2 = model.forward(input_ids_2, attention_mask_2)\n",
    "                diff = (out1 - out2).squeeze()\n",
    "                diff = torch.sigmoid(diff)\n",
    "                loss = loss_fn(diff, y.float().to(device))\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        if(tune):\n",
    "            tune.report(loss=val_loss)\n",
    "\n",
    "        logger.info(\"[%d] val loss: %.10f\" % (epoch+1, val_loss))\n",
    "        return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEGxtzDEPLDQ"
   },
   "outputs": [],
   "source": [
    "def train_tune(config, data_dir=\"./data\"):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = ProductRanker(config[\"l1\"],config[\"l2\"]).to(device)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], betas=(0.9, 0.999), eps=1e-08, weight_decay=0,\n",
    "                                amsgrad=False)\n",
    "    \n",
    "    train_set, val_set = dataPreparation(data_dir)\n",
    "    dataloader_train = createDataloader(train_set, config)\n",
    "    dataloader_val = createDataloader(val_set, config)\n",
    "\n",
    "    train(model, optimizer, dataloader_train, 0, tune=True)\n",
    "    val(model, loss_fn, dataloader_val, 0, tune=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xLrYjbv9QJx"
   },
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mrEAanp9OFO"
   },
   "outputs": [],
   "source": [
    "def tune_hyperparameters():\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    config = {\n",
    "        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([2, 4, 8, 16])\n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=4,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    reporter = CLIReporter(\n",
    "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        partial(train_tune, data_dir=data_dir),\n",
    "        resources_per_trial={\"cpu\": 12, \"gpu\": 1},\n",
    "        config=config,\n",
    "        num_samples=10,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YStntQVRDUGM"
   },
   "outputs": [],
   "source": [
    "def tune_lr():\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    config = {\n",
    "        'l1': 256,\n",
    "        'l2': 8,\n",
    "        'lr': tune.loguniform(1e-4, 1e-1),\n",
    "        'batch_size': 4}\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=5,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    reporter = CLIReporter(\n",
    "        metric_columns=[\"loss\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        partial(train_tune, data_dir=data_dir),\n",
    "        resources_per_trial={\"cpu\": 12, \"gpu\": 1},\n",
    "        local_dir=\"./ray_results\",\n",
    "        config=config,\n",
    "        num_samples=5,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    best_checkpoint = result.get_best_checkpoint(best_trial, metric=\"loss\", mode=\"min\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmPbsO6w9c7T"
   },
   "outputs": [],
   "source": [
    "def train_model(epochs, config, from_checkpoint=False, path=None):\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    train(epochs, config, data_dir, from_checkpoint, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQ6Va6c59fP1"
   },
   "outputs": [],
   "source": [
    "def main(epochs, train_bool=True, validate_bool=True, data_dir=\"./data\", saving=False, load=False):\n",
    "    getJSON()\n",
    "    train_set, val_set = dataPreparation()\n",
    "    dataloader_train = createDataloader(train_set, config)\n",
    "    dataloader_val = createDataloader(val_set, config)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if load:\n",
    "            current_model_filename = './outputs/model_' + str(epoch+1) + '.pth'\n",
    "            if os.path.exists(current_model_filename):\n",
    "                    checkpoint = torch.load(current_model_filename)\n",
    "                    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            else:\n",
    "                logger.info(f\"model: {current_model_filename} doesnt exist\")\n",
    "        \n",
    "        logger.info(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "        if train_bool:\n",
    "            logger.info(\"Starting training loop...\")\n",
    "            train(model, loss_fn, optimizer, dataloader_train, epoch, saving=saving, tune=False)\n",
    "        \n",
    "        if validate_bool:\n",
    "            logger.info(\"Starting validation loop...\")\n",
    "            val(model, loss_fn, dataloader_val, epoch, tune=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C7vVclae-Ze6",
    "outputId": "747cb3ef-f2b6-4e92-9fa6-d8fa362eacfa"
   },
   "outputs": [],
   "source": [
    "main(5, saving=True, load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GBxiaLnjXIfV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
