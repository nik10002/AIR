{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MwEr46hqDtJK",
    "outputId": "311084f7-f109-4a2f-8087-0c085942a63c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wget in /home/niklas/.local/lib/python3.10/site-packages (3.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/niklas/.local/lib/python3.10/site-packages (4.25.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/niklas/.local/lib/python3.10/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/niklas/.local/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/niklas/.local/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/niklas/.local/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/niklas/.local/lib/python3.10/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/niklas/.local/lib/python3.10/site-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/niklas/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ray in /home/niklas/.local/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: click>=7.0 in /usr/lib/python3/dist-packages (from ray) (8.0.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray) (3.8.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/niklas/.local/lib/python3.10/site-packages (from ray) (1.0.4)\n",
      "Requirement already satisfied: virtualenv>=20.0.24 in /usr/local/lib/python3.10/dist-packages (from ray) (20.16.7)\n",
      "Requirement already satisfied: packaging in /home/niklas/.local/lib/python3.10/site-packages (from ray) (21.3)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /home/niklas/.local/lib/python3.10/site-packages (from ray) (1.23.4)\n",
      "Requirement already satisfied: jsonschema in /home/niklas/.local/lib/python3.10/site-packages (from ray) (4.17.0)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from ray) (5.4.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray) (2.28.1)\n",
      "Requirement already satisfied: aiosignal in /home/niklas/.local/lib/python3.10/site-packages (from ray) (1.3.1)\n",
      "Requirement already satisfied: attrs in /home/niklas/.local/lib/python3.10/site-packages (from ray) (22.1.0)\n",
      "Requirement already satisfied: frozenlist in /home/niklas/.local/lib/python3.10/site-packages (from ray) (1.3.3)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /home/niklas/.local/lib/python3.10/site-packages (from ray) (4.21.12)\n",
      "Requirement already satisfied: grpcio>=1.42.0 in /home/niklas/.local/lib/python3.10/site-packages (from ray) (1.51.1)\n",
      "Requirement already satisfied: platformdirs<3,>=2.4 in /home/niklas/.local/lib/python3.10/site-packages (from virtualenv>=20.0.24->ray) (2.5.4)\n",
      "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.0.24->ray) (0.3.6)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/niklas/.local/lib/python3.10/site-packages (from jsonschema->ray) (0.19.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging->ray) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (2.1.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/niklas/.local/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/niklas/.local/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/niklas/.local/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/niklas/.local/lib/python3.10/site-packages (from scikit-learn) (1.23.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/niklas/.local/lib/python3.10/site-packages (from scikit-learn) (1.9.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install wget\n",
    "!pip install transformers\n",
    "!pip install ray\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "N1mZmWzU18_W"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import wget\n",
    "import gzip\n",
    "import html\n",
    "import tarfile\n",
    "import logging\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, AdamW, BertTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mCd6h66j7mKf"
   },
   "outputs": [],
   "source": [
    "class ProductRanker(nn.Module):\n",
    "    def __init__(self, l1=128, l2=32):\n",
    "        super(ProductRanker, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.ranker = nn.Sequential(\n",
    "            nn.Linear(768, l1),\n",
    "            nn.BatchNorm1d(l1, affine=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(l1, l2),\n",
    "            nn.BatchNorm1d(l2, affine=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(l2,1)\n",
    "        )\n",
    "\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        logits = self.ranker(last_hidden_state_cls)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tA9ILTLDDmkE",
    "outputId": "b5312e23-ee29-4c5c-891f-66d677b616d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new HTTPS connection (1): huggingface.co:443\n",
      "https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#Constants\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config = {'l1': 128, 'l2': 32, 'lr': 0.0005, 'batch_size': 100}\n",
    "model = ProductRanker(config[\"l1\"],config[\"l2\"]).to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], betas=(0.9, 0.999), eps=1e-08, weight_decay=0,\n",
    "                            amsgrad=False)\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "if not os.path.exists(\"./data\"):\n",
    "    os.makedirs(\"./data\")\n",
    "if not os.path.exists(\"./outputs\"):\n",
    "    os.makedirs(\"./outputs\")\n",
    "\n",
    "logging.basicConfig(filename='./outputs/training.log', filemode='w', format='%(asctime)s %(message)s', level=logging.DEBUG, force=True)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "g35D5YJf3Ila"
   },
   "outputs": [],
   "source": [
    "def downloadData(url, path):\n",
    "    return wget.download(url, out=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4-NKG6m3OjZ"
   },
   "outputs": [],
   "source": [
    "def loadJson(filename):\n",
    "    data = []\n",
    "    with gzip.open(filename, \"rt\") as f:\n",
    "        for line in f:\n",
    "            stripped = line.strip()\n",
    "            stripped = stripped.replace(\"\\'\", \"\\\"\")\n",
    "            try:\n",
    "                data.append(json.loads(stripped))\n",
    "            except json.decoder.JSONDecodeError:\n",
    "                continue \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZatCtPg3QRz"
   },
   "outputs": [],
   "source": [
    "def filterJson(data, extrema_dict):\n",
    "    filtered = []\n",
    "    for entry in data:\n",
    "        if not isGoodJsonEntry(entry):\n",
    "            continue\n",
    "        \n",
    "        sales_rank_key = list(entry['salesRank'].keys())[0]\n",
    "        sales_rank_value = entry['salesRank'][sales_rank_key]\n",
    "        min = extrema_dict[sales_rank_key][0]\n",
    "        max = extrema_dict[sales_rank_key][1]\n",
    "\n",
    "        tmp = {}\n",
    "        if min == max:\n",
    "            tmp['salesRank'] = 0.5\n",
    "        else:\n",
    "            tmp['salesRank'] = (sales_rank_value - min) / (max - min)\n",
    "        tmp['description'] = html.unescape(entry['description'])\n",
    "        filtered.append(tmp)\n",
    "\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFHxm4vW3TU6"
   },
   "outputs": [],
   "source": [
    "def getExtremaDict(data):\n",
    "    extrema_dict = {}\n",
    "    for entry in data:\n",
    "        if not isGoodJsonEntry(entry):\n",
    "            continue\n",
    "\n",
    "        sales_rank_key = list(entry['salesRank'].keys())[0]\n",
    "        sales_rank_value = entry['salesRank'][sales_rank_key]\n",
    "        if sales_rank_key not in extrema_dict:\n",
    "            extrema_dict[sales_rank_key] = [sales_rank_value, sales_rank_value] \n",
    "        else:\n",
    "            if extrema_dict[sales_rank_key][0] > sales_rank_value:\n",
    "                extrema_dict[sales_rank_key][0] = sales_rank_value\n",
    "            elif extrema_dict[sales_rank_key][1] < sales_rank_value:\n",
    "                extrema_dict[sales_rank_key][1] = sales_rank_value\n",
    "\n",
    "    return extrema_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUCHiFQx3Vjo"
   },
   "outputs": [],
   "source": [
    "def isGoodJsonEntry(entry):\n",
    "    if 'description' not in entry.keys():\n",
    "        return False\n",
    "    if len(entry['description']) == 0:\n",
    "        return False\n",
    "    if 'salesRank' not in entry.keys():\n",
    "        return False\n",
    "    if len(entry['salesRank']) == 0:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iYBaprFg3YFW"
   },
   "outputs": [],
   "source": [
    "def safeJson(filtered, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for object in filtered:\n",
    "            json.dump(object, f)\n",
    "            f.write(os.linesep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qxt6iaSp3Zmv"
   },
   "outputs": [],
   "source": [
    "def getJSON(data_dir=\"./data\"):\n",
    "    filename_meta = os.path.join(data_dir, \"meta_Electronics.json.gz\")\n",
    "    filename_filtered_meta = os.path.join(data_dir, \"filtered_meta_Electronics.tar.gz\")\n",
    "    filename_destination_meta = os.path.join(data_dir, \"filtered_meta_Electronics.json\")\n",
    "\n",
    "    url_meta = \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/meta_Electronics.json.gz\"\n",
    "    \n",
    "    if os.path.exists(filename_destination_meta):\n",
    "        return\n",
    "\n",
    "    if os.path.exists(filename_filtered_meta):\n",
    "        file = tarfile.open(filename_filtered_meta, \"r:gz\")\n",
    "        file.extractall(data_dir)\n",
    "        file.close()\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(filename_meta):\n",
    "        downloadData(url_meta, filename_meta)\n",
    "    \n",
    "    data = loadJson(filename_meta)\n",
    "    extrema_dict = getExtremaDict(data)\n",
    "    filtered = filterJson(data, extrema_dict)\n",
    "    safeJson(filtered, filename_destination_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGoogleWord2Vec():\n",
    "    return api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "model_word2vec = loadGoogleWord2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averagePollingDocument(model_word2vec, data_dir=\"./data\"):\n",
    "    path = os.path.join(data_dir, \"filtered_meta_Electronics.json\")\n",
    "    df = pd.read_json(path, lines=True)\n",
    "    \n",
    "    \n",
    "    len_vec = len(model_word2vec[\"hello\"])\n",
    "    \n",
    "    documents = dict()\n",
    "    for index, row in df.iterrows():\n",
    "        tmp = torch.zeros(len_vec)\n",
    "        description = utils.simple_preprocess(row['description'])\n",
    "        for word in description:\n",
    "            try:\n",
    "                tmp = tmp.add(torch.tensor(model_word2vec[word]))\n",
    "            except KeyError:\n",
    "                continue\n",
    "        documents[index] = tmp.div(len(description))\n",
    "        \n",
    "    return documents\n",
    "\n",
    "documents = averagePollingDocument(model_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor(0.4788), 8750), (tensor(0.4092), 15011), (tensor(0.4070), 17988)]\n"
     ]
    }
   ],
   "source": [
    "def averagePollingQuery(query, model_word2vec):\n",
    "    query = utils.simple_preprocess(query)\n",
    "    len_vec = len(model_word2vec[\"hello\"])\n",
    "    \n",
    "    avg_query = torch.zeros(len_vec)\n",
    "    for word in query:\n",
    "        try:\n",
    "            avg_query += avg_query.add(torch.tensor(model_word2vec[word]))\n",
    "        except KeyError:\n",
    "            continue\n",
    "    avg_query = avg_query / len(query)\n",
    "    return avg_query\n",
    "\n",
    "avg_query = averagePollingQuery(\"test test HELLO\", model_word2vec)\n",
    "all = [(cos(avg_query, value), indx) for indx, value in documents.items()]\n",
    "all.sort(key=lambda x: -x[0].item())\n",
    "three_most_similar = all[:3]\n",
    "print(three_most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMavKxtY8Bv6"
   },
   "outputs": [],
   "source": [
    "def dataPreparation(data_dir=\"./data\"):\n",
    "    path = os.path.join(data_dir, \"filtered_meta_Electronics.json\")\n",
    "    df = pd.read_json(path, lines=True).sample(n = (150 + 75 + 5625))\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    tokenized_set = []\n",
    "    for index, row in df.iterrows():\n",
    "        tokenized_set.append([tokenizer.encode_plus(row['description'], truncation = True, return_tensors=\"pt\",\n",
    "                                                    max_length=512, pad_to_max_length=True), row['salesRank']])\n",
    "    return torch.utils.data.random_split(tokenized_set, [150, 75, 5625])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-y6YFzn8SWW"
   },
   "outputs": [],
   "source": [
    "def createDataloader(dataset, config, shuffle=True, test=False):\n",
    "    labeled_set = []\n",
    "    if test:\n",
    "        for i, doc1 in enumerate(dataset):\n",
    "            labeled_set.append([[doc1[0]], doc1[1]])\n",
    "    else:\n",
    "        for i, doc1 in enumerate(dataset):\n",
    "            for j, doc2 in enumerate(dataset):\n",
    "                if i != j:\n",
    "                    label = 0.0\n",
    "                    if doc1[1] > doc2[1]:\n",
    "                        label = 1.0\n",
    "                    if doc1[1] == doc2[1]:\n",
    "                        label = 0.5\n",
    "\n",
    "                    labeled_set.append([[doc1[0], doc2[0]], label])\n",
    "    return torch.utils.data.DataLoader(labeled_set, batch_size=int(config[\"batch_size\"]), num_workers=2, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7k5M2SU8Zw8"
   },
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, dataloader, epoch, saving=False, tune=False):\n",
    "    running_loss = 0.0\n",
    "    epoch_steps = 0\n",
    "\n",
    "    train_loss = 0.0\n",
    "    acc_tp_tn = 0\n",
    "    acc_total = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for X,y in dataloader:\n",
    "        input_ids_1 = X[0]['input_ids'].squeeze().to(device)\n",
    "        attention_mask_1 = X[0]['attention_mask'].squeeze().to(device)\n",
    "        input_ids_2 = X[1]['input_ids'].squeeze().to(device)\n",
    "        attention_mask_2 = X[1]['attention_mask'].squeeze().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out1 = model.forward(input_ids_1, attention_mask_1)\n",
    "        out2 = model.forward(input_ids_2, attention_mask_2)\n",
    "        diff = (out1 - out2).squeeze()\n",
    "        diff = torch.sigmoid(diff).to(device)\n",
    "        loss = loss_fn(diff, y.float().to(device))\n",
    "        \n",
    "        for i in range(len(diff)):\n",
    "            if (diff[i] >= 0.5 and y[i] == 1) or (diff[i] < 0.5 and y[i] == 0):\n",
    "                acc_tp_tn += 1\n",
    "            acc_total +=1\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    logger.info(\"[%d] train loss: %.10f\" % (epoch + 1, train_loss / len(dataloader)))\n",
    "    logging.info(\"[%d] train accuracy: %.10f\" % (epoch + 1, acc_tp_tn / acc_total))\n",
    "\n",
    "    if saving:\n",
    "        path = './outputs/model_' + str(epoch+1) + '.pth'\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }, path)\n",
    "        logger.info(\"Model saved..\")\n",
    "\n",
    "    if tune:\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "    logger.info(\"Training finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4-vkMnZ8cq7"
   },
   "outputs": [],
   "source": [
    "def val(model, loss_fn, dataloader, epoch, tune=False):\n",
    "        val_loss = 0.0\n",
    "        acc_tp_tn = 0\n",
    "        acc_total = 0\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        for X, y in dataloader:\n",
    "            with torch.no_grad():\n",
    "                input_ids_1 = X[0]['input_ids'].squeeze().to(device)\n",
    "                attention_mask_1 = X[0]['attention_mask'].squeeze().to(device)\n",
    "                input_ids_2 = X[1]['input_ids'].squeeze().to(device)\n",
    "                attention_mask_2 = X[1]['attention_mask'].squeeze().to(device)\n",
    "                out1 = model.forward(input_ids_1, attention_mask_1)\n",
    "                out2 = model.forward(input_ids_2, attention_mask_2)\n",
    "                diff = (out1 - out2).squeeze()\n",
    "                diff = torch.sigmoid(diff)\n",
    "                loss = loss_fn(diff, y.float().to(device))\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                \n",
    "                for i in range(len(diff)):\n",
    "                    if (diff[i] >= 0.5 and y[i] == 1) or (diff[i] < 0.5 and y[i] == 0):\n",
    "                        acc_tp_tn += 1\n",
    "                    acc_total +=1\n",
    "\n",
    "        if(tune):\n",
    "            tune.report(loss=val_loss)\n",
    "\n",
    "        logger.info(\"[%d] val loss: %.10f\" % (epoch+1, val_loss/len(dataloader)))\n",
    "        logging.info(\"[%d] val acc: %.10f\" % (epoch + 1, acc_tp_tn/acc_total))\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgV_KMYRllIi"
   },
   "outputs": [],
   "source": [
    "def test(model, dataloader, epoch):\n",
    "\n",
    "    mse_ = 0.0\n",
    "    model.eval()\n",
    "    \n",
    "    for X, y in dataloader:\n",
    "        with torch.no_grad():\n",
    "            input_ids = X[0]['input_ids'].squeeze().to(device)\n",
    "            attention_mask = X[0]['attention_mask'].squeeze().to(device)\n",
    "            out = model.forward(input_ids, attention_mask).squeeze().cpu().numpy()\n",
    "            mse_ += mean_squared_error(y, out)\n",
    "    \n",
    "    logger.info(\"[%d] val loss: %.10f\" % (epoch+1, mse_/len(dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEGxtzDEPLDQ"
   },
   "outputs": [],
   "source": [
    "def train_tune(config, data_dir=\"./data\"):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = ProductRanker(config[\"l1\"],config[\"l2\"]).to(device)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], betas=(0.9, 0.999), eps=1e-08, weight_decay=0,\n",
    "                                amsgrad=False)\n",
    "    \n",
    "    train_set, val_set = dataPreparation(data_dir)\n",
    "    dataloader_train = createDataloader(train_set, config)\n",
    "    dataloader_val = createDataloader(val_set, config)\n",
    "\n",
    "    train(model, optimizer, dataloader_train, 0, tune=True)\n",
    "    val(model, loss_fn, dataloader_val, 0, tune=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mrEAanp9OFO"
   },
   "outputs": [],
   "source": [
    "def tune_hyperparameters():\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    config = {\n",
    "        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([2, 4, 8, 16])\n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=4,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    reporter = CLIReporter(\n",
    "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        partial(train_tune, data_dir=data_dir),\n",
    "        resources_per_trial={\"cpu\": 12, \"gpu\": 1},\n",
    "        config=config,\n",
    "        num_samples=10,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YStntQVRDUGM"
   },
   "outputs": [],
   "source": [
    "def tune_lr():\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    config = {\n",
    "        'l1': 256,\n",
    "        'l2': 8,\n",
    "        'lr': tune.loguniform(1e-4, 1e-1),\n",
    "        'batch_size': 4}\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=5,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    reporter = CLIReporter(\n",
    "        metric_columns=[\"loss\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        partial(train_tune, data_dir=data_dir),\n",
    "        resources_per_trial={\"cpu\": 12, \"gpu\": 1},\n",
    "        local_dir=\"./ray_results\",\n",
    "        config=config,\n",
    "        num_samples=5,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    best_checkpoint = result.get_best_checkpoint(best_trial, metric=\"loss\", mode=\"min\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmPbsO6w9c7T"
   },
   "outputs": [],
   "source": [
    "def train_model(epochs, config, from_checkpoint=False, path=None):\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    train(epochs, config, data_dir, from_checkpoint, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQ6Va6c59fP1"
   },
   "outputs": [],
   "source": [
    "def main(epochs, train_bool=True, validate_bool=True, test_bool=True, data_dir=\"./data\", saving=False, load=False):\n",
    "    getJSON()\n",
    "    train_set, val_set, test_set = dataPreparation()\n",
    "    dataloader_train = createDataloader(train_set, config)\n",
    "    dataloader_val = createDataloader(val_set, config)\n",
    "    dataloader_test = createDataloader(test_set, config, test=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if load:\n",
    "            current_model_filename = './outputs/model_' + str(epoch+1) + '.pth'\n",
    "            if os.path.exists(current_model_filename):\n",
    "                    checkpoint = torch.load(current_model_filename)\n",
    "                    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            else:\n",
    "                logger.info(f\"model: {current_model_filename} doesnt exist\")\n",
    "        \n",
    "        logger.info(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "        if train_bool:\n",
    "            logger.info(\"Starting training loop...\")\n",
    "            train(model, loss_fn, optimizer, dataloader_train, epoch, saving=saving, tune=False)\n",
    "        \n",
    "        if validate_bool:\n",
    "            logger.info(\"Starting validation loop...\")\n",
    "            val(model, loss_fn, dataloader_val, epoch, tune=False)\n",
    "\n",
    "        if epoch == epochs - 1:\n",
    "            if test_bool:\n",
    "                logger.info(\"Starting testing loop...\")\n",
    "                test(model, dataloader_test, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C7vVclae-Ze6",
    "outputId": "8e959e24-d0a5-44e9-d177-ade3880bf22c"
   },
   "outputs": [],
   "source": [
    "main(5, train_bool=False, validate_bool=False, saving=False, load=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
