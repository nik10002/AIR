{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MwEr46hqDtJK",
    "outputId": "311084f7-f109-4a2f-8087-0c085942a63c"
   },
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "!pip install transformers\n",
    "!pip install ray\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1mZmWzU18_W"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import wget\n",
    "import gzip\n",
    "import html\n",
    "import tarfile\n",
    "import logging\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, AdamW, BertTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import sample\n",
    "\n",
    "from functools import partial\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCd6h66j7mKf"
   },
   "outputs": [],
   "source": [
    "class ProductRanker(nn.Module):\n",
    "    def __init__(self, l1=128, l2=32):\n",
    "        super(ProductRanker, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.ranker = nn.Sequential(\n",
    "            nn.Linear(768, l1),\n",
    "            nn.BatchNorm1d(l1, affine=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(l1, l2),\n",
    "            nn.BatchNorm1d(l2, affine=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(l2,1)\n",
    "        )\n",
    "\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        logits = self.ranker(last_hidden_state_cls)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tA9ILTLDDmkE",
    "outputId": "b5312e23-ee29-4c5c-891f-66d677b616d6"
   },
   "outputs": [],
   "source": [
    "#Constants\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config = {'l1': 128, 'l2': 32, 'lr': 0.0005, 'batch_size': 100}\n",
    "model = ProductRanker(config[\"l1\"],config[\"l2\"]).to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], betas=(0.9, 0.999), eps=1e-08, weight_decay=0,\n",
    "                            amsgrad=False)\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "if not os.path.exists(\"./data\"):\n",
    "    os.makedirs(\"./data\")\n",
    "if not os.path.exists(\"./outputs\"):\n",
    "    os.makedirs(\"./outputs\")\n",
    "\n",
    "logging.basicConfig(filename='./outputs/training.log', filemode='w', format='%(asctime)s %(message)s', level=logging.DEBUG, force=True)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g35D5YJf3Ila"
   },
   "outputs": [],
   "source": [
    "def downloadData(url, path):\n",
    "    return wget.download(url, out=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4-NKG6m3OjZ"
   },
   "outputs": [],
   "source": [
    "def loadJson(filename):\n",
    "    data = []\n",
    "    with gzip.open(filename, \"rt\") as f:\n",
    "        for line in f:\n",
    "            stripped = line.strip()\n",
    "            stripped = stripped.replace(\"\\'\", \"\\\"\")\n",
    "            try:\n",
    "                data.append(json.loads(stripped))\n",
    "            except json.decoder.JSONDecodeError:\n",
    "                continue \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZatCtPg3QRz"
   },
   "outputs": [],
   "source": [
    "def filterJson(data, extrema_dict):\n",
    "    filtered = []\n",
    "    for entry in data:\n",
    "        if not isGoodJsonEntry(entry):\n",
    "            continue\n",
    "        \n",
    "        sales_rank_key = list(entry['salesRank'].keys())[0]\n",
    "        sales_rank_value = entry['salesRank'][sales_rank_key]\n",
    "        min = extrema_dict[sales_rank_key][0]\n",
    "        max = extrema_dict[sales_rank_key][1]\n",
    "\n",
    "        tmp = {}\n",
    "        if min == max:\n",
    "            tmp['salesRank'] = 0.5\n",
    "        else:\n",
    "            tmp['salesRank'] = (sales_rank_value - min) / (max - min)\n",
    "        tmp['description'] = html.unescape(entry['description'])\n",
    "        filtered.append(tmp)\n",
    "\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFHxm4vW3TU6"
   },
   "outputs": [],
   "source": [
    "def getExtremaDict(data):\n",
    "    extrema_dict = {}\n",
    "    for entry in data:\n",
    "        if not isGoodJsonEntry(entry):\n",
    "            continue\n",
    "\n",
    "        sales_rank_key = list(entry['salesRank'].keys())[0]\n",
    "        sales_rank_value = entry['salesRank'][sales_rank_key]\n",
    "        if sales_rank_key not in extrema_dict:\n",
    "            extrema_dict[sales_rank_key] = [sales_rank_value, sales_rank_value] \n",
    "        else:\n",
    "            if extrema_dict[sales_rank_key][0] > sales_rank_value:\n",
    "                extrema_dict[sales_rank_key][0] = sales_rank_value\n",
    "            elif extrema_dict[sales_rank_key][1] < sales_rank_value:\n",
    "                extrema_dict[sales_rank_key][1] = sales_rank_value\n",
    "\n",
    "    return extrema_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUCHiFQx3Vjo"
   },
   "outputs": [],
   "source": [
    "def isGoodJsonEntry(entry):\n",
    "    if 'description' not in entry.keys():\n",
    "        return False\n",
    "    if len(entry['description']) == 0:\n",
    "        return False\n",
    "    if 'salesRank' not in entry.keys():\n",
    "        return False\n",
    "    if len(entry['salesRank']) == 0:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iYBaprFg3YFW"
   },
   "outputs": [],
   "source": [
    "def safeJson(filtered, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for object in filtered:\n",
    "            json.dump(object, f)\n",
    "            f.write(os.linesep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qxt6iaSp3Zmv"
   },
   "outputs": [],
   "source": [
    "def getJSON(data_dir=\"./data\"):\n",
    "    filename_meta = os.path.join(data_dir, \"meta_Electronics.json.gz\")\n",
    "    filename_filtered_meta = os.path.join(data_dir, \"filtered_meta_Electronics.tar.gz\")\n",
    "    filename_destination_meta_train = os.path.join(data_dir, \"filtered_meta_Electronics_train.json\")\n",
    "    filename_destination_meta_test = os.path.join(data_dir, \"filtered_meta_Electronics_test.json\")\n",
    "\n",
    "    url_meta = \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/meta_Electronics.json.gz\"\n",
    "    \n",
    "    if os.path.exists(filename_destination_meta_train) and os.path.exists(filename_destination_meta_test):\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(filename_meta):\n",
    "        downloadData(url_meta, filename_meta)\n",
    "    \n",
    "    data = loadJson(filename_meta)\n",
    "    extrema_dict = getExtremaDict(data)\n",
    "    filtered = filterJson(data, extrema_dict)\n",
    "    train_set, test_set = torch.utils.data.random_split(filtered, [len(filtered)-5625, 5625])\n",
    "    safeJson(train_set, filename_destination_meta_train)\n",
    "    safeJson(test_set, filename_destination_meta_test)\n",
    "    \n",
    "getJSON()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGoogleWord2Vec():\n",
    "    return api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averagePollingDocument(model_word2vec, data_dir=\"./data\"):\n",
    "    path = os.path.join(data_dir, \"filtered_meta_Electronics_test.json\")\n",
    "    df = pd.read_json(path, lines=True) \n",
    "    word_set = set()\n",
    "    \n",
    "    len_vec = len(model_word2vec[\"hello\"])\n",
    "    \n",
    "    documents = dict()\n",
    "    for index, row in df.iterrows():\n",
    "        tmp = torch.zeros(len_vec)\n",
    "        description = utils.simple_preprocess(row['description'])\n",
    "        for word in description:\n",
    "            try:\n",
    "                tmp = tmp.add(torch.tensor(model_word2vec[word]))\n",
    "                word_set.add(word)\n",
    "            except KeyError:\n",
    "                continue\n",
    "        documents[index] = tmp.div(len(description))\n",
    "        \n",
    "    return documents, list(word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averagePollingQuery(query, model_word2vec):\n",
    "    query = utils.simple_preprocess(query)\n",
    "    len_vec = len(model_word2vec[\"hello\"])\n",
    "    \n",
    "    avg_query = torch.zeros(len_vec)\n",
    "    for word in query:\n",
    "        try:\n",
    "            avg_query += avg_query.add(torch.tensor(model_word2vec[word]))\n",
    "        except KeyError:\n",
    "            continue\n",
    "    avg_query = avg_query / len(query)\n",
    "    return avg_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRandomQuery(word_set, x=5):\n",
    "    return ' '.join(sample(word_set, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMostSimilair(avg_query, documents, x=100):\n",
    "    cosine_sim = [(cos(avg_query, value), indx) for indx, value in documents.items()]\n",
    "    cosine_sim.sort(key=lambda y: -y[0].item())\n",
    "    most_similair = cosine_sim[:x]\n",
    "    most_similair = [x[1] for x in most_similair]\n",
    "    return most_similair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMavKxtY8Bv6"
   },
   "outputs": [],
   "source": [
    "def dataPreparation(data_dir=\"./data\"):\n",
    "    path = os.path.join(data_dir, \"filtered_meta_Electronics_train.json\")\n",
    "    df = pd.read_json(path, lines=True).sample(n = (225))\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    tokenized_set = []\n",
    "    for index, row in df.iterrows():\n",
    "        tokenized_set.append([tokenizer.encode_plus(row['description'], truncation = True, return_tensors=\"pt\",\n",
    "                                                    max_length=512, pad_to_max_length=True), row['salesRank']])\n",
    "    return torch.utils.data.random_split(tokenized_set, [150, 75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-y6YFzn8SWW"
   },
   "outputs": [],
   "source": [
    "def createDataloader(dataset, config, shuffle=True, test=False):\n",
    "    labeled_set = []\n",
    "    if test:\n",
    "        for i, doc1 in enumerate(dataset):\n",
    "            labeled_set.append([[doc1[0]], doc1[1]])\n",
    "    else:\n",
    "        for i, doc1 in enumerate(dataset):\n",
    "            for j, doc2 in enumerate(dataset):\n",
    "                if i != j:\n",
    "                    label = 0.0\n",
    "                    if doc1[1] > doc2[1]:\n",
    "                        label = 1.0\n",
    "                    if doc1[1] == doc2[1]:\n",
    "                        label = 0.5\n",
    "\n",
    "                    labeled_set.append([[doc1[0], doc2[0]], label])\n",
    "    return torch.utils.data.DataLoader(labeled_set, batch_size=int(config[\"batch_size\"]), num_workers=2, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataloaderTest(most_similair, data_dir=\"./data\"):\n",
    "    path = os.path.join(data_dir, \"filtered_meta_Electronics_train.json\")\n",
    "    df = pd.read_json(path, lines=True)\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    tokenized_set = []\n",
    "    for index, row in df.iterrows():\n",
    "        if index in most_similair:\n",
    "            tokenized_set.append([tokenizer.encode_plus(row['description'], truncation = True, return_tensors=\"pt\",\n",
    "                                                    max_length=512, pad_to_max_length=True), row['salesRank']])\n",
    "\n",
    "    return createDataloader(tokenized_set, config, test=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7k5M2SU8Zw8"
   },
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, dataloader, epoch, saving=False, tune=False):\n",
    "    running_loss = 0.0\n",
    "    epoch_steps = 0\n",
    "\n",
    "    train_loss = 0.0\n",
    "    acc_tp_tn = 0\n",
    "    acc_total = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for X,y in dataloader:\n",
    "        input_ids_1 = X[0]['input_ids'].squeeze().to(device)\n",
    "        attention_mask_1 = X[0]['attention_mask'].squeeze().to(device)\n",
    "        input_ids_2 = X[1]['input_ids'].squeeze().to(device)\n",
    "        attention_mask_2 = X[1]['attention_mask'].squeeze().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out1 = model.forward(input_ids_1, attention_mask_1)\n",
    "        out2 = model.forward(input_ids_2, attention_mask_2)\n",
    "        diff = (out1 - out2).squeeze()\n",
    "        diff = torch.sigmoid(diff).to(device)\n",
    "        loss = loss_fn(diff, y.float().to(device))\n",
    "        \n",
    "        for i in range(len(diff)):\n",
    "            if (diff[i] >= 0.5 and y[i] == 1) or (diff[i] < 0.5 and y[i] == 0):\n",
    "                acc_tp_tn += 1\n",
    "            acc_total +=1\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    logger.info(\"[%d] train loss: %.10f\" % (epoch + 1, train_loss / len(dataloader)))\n",
    "    logging.info(\"[%d] train accuracy: %.10f\" % (epoch + 1, acc_tp_tn / acc_total))\n",
    "\n",
    "    if saving:\n",
    "        path = './outputs/model_' + str(epoch+1) + '.pth'\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }, path)\n",
    "        logger.info(\"Model saved..\")\n",
    "\n",
    "    if tune:\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "    logger.info(\"Training finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4-vkMnZ8cq7"
   },
   "outputs": [],
   "source": [
    "def val(model, loss_fn, dataloader, epoch, tune=False):\n",
    "        val_loss = 0.0\n",
    "        acc_tp_tn = 0\n",
    "        acc_total = 0\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        for X, y in dataloader:\n",
    "            with torch.no_grad():\n",
    "                input_ids_1 = X[0]['input_ids'].squeeze().to(device)\n",
    "                attention_mask_1 = X[0]['attention_mask'].squeeze().to(device)\n",
    "                input_ids_2 = X[1]['input_ids'].squeeze().to(device)\n",
    "                attention_mask_2 = X[1]['attention_mask'].squeeze().to(device)\n",
    "                out1 = model.forward(input_ids_1, attention_mask_1)\n",
    "                out2 = model.forward(input_ids_2, attention_mask_2)\n",
    "                diff = (out1 - out2).squeeze()\n",
    "                diff = torch.sigmoid(diff)\n",
    "                loss = loss_fn(diff, y.float().to(device))\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                \n",
    "                for i in range(len(diff)):\n",
    "                    if (diff[i] >= 0.5 and y[i] == 1) or (diff[i] < 0.5 and y[i] == 0):\n",
    "                        acc_tp_tn += 1\n",
    "                    acc_total +=1\n",
    "\n",
    "        if(tune):\n",
    "            tune.report(loss=val_loss)\n",
    "\n",
    "        logger.info(\"[%d] val loss: %.10f\" % (epoch+1, val_loss/len(dataloader)))\n",
    "        logging.info(\"[%d] val acc: %.10f\" % (epoch + 1, acc_tp_tn/acc_total))\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEGxtzDEPLDQ"
   },
   "outputs": [],
   "source": [
    "def train_tune(config, data_dir=\"./data\"):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = ProductRanker(config[\"l1\"],config[\"l2\"]).to(device)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], betas=(0.9, 0.999), eps=1e-08, weight_decay=0,\n",
    "                                amsgrad=False)\n",
    "    \n",
    "    train_set, val_set = dataPreparation(data_dir)\n",
    "    dataloader_train = createDataloader(train_set, config)\n",
    "    dataloader_val = createDataloader(val_set, config)\n",
    "\n",
    "    train(model, optimizer, dataloader_train, 0, tune=True)\n",
    "    val(model, loss_fn, dataloader_val, 0, tune=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mrEAanp9OFO"
   },
   "outputs": [],
   "source": [
    "def tune_hyperparameters():\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    config = {\n",
    "        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([2, 4, 8, 16])\n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=4,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    reporter = CLIReporter(\n",
    "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        partial(train_tune, data_dir=data_dir),\n",
    "        resources_per_trial={\"cpu\": 12, \"gpu\": 1},\n",
    "        config=config,\n",
    "        num_samples=10,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YStntQVRDUGM"
   },
   "outputs": [],
   "source": [
    "def tune_lr():\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    config = {\n",
    "        'l1': 256,\n",
    "        'l2': 8,\n",
    "        'lr': tune.loguniform(1e-4, 1e-1),\n",
    "        'batch_size': 4}\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=5,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    reporter = CLIReporter(\n",
    "        metric_columns=[\"loss\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        partial(train_tune, data_dir=data_dir),\n",
    "        resources_per_trial={\"cpu\": 12, \"gpu\": 1},\n",
    "        local_dir=\"./ray_results\",\n",
    "        config=config,\n",
    "        num_samples=5,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    best_checkpoint = result.get_best_checkpoint(best_trial, metric=\"loss\", mode=\"min\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmPbsO6w9c7T"
   },
   "outputs": [],
   "source": [
    "def train_model(epochs, config, from_checkpoint=False, path=None):\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    train(epochs, config, data_dir, from_checkpoint, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MRR(y, rankings):\n",
    "    y_dict = {}\n",
    "    for i in range(len(y)):\n",
    "        y_dict[i] = y[i]\n",
    "        \n",
    "    y_dict = {key: rank for rank, key in enumerate(sorted(y_dict, key=y_dict.get, reverse=True), 1)}\n",
    "    mrr_ret = 0\n",
    "    for i in range(len(y)):\n",
    "        mrr_ret += y[i] / rankings[i]\n",
    "    \n",
    "    #print(y_dict)\n",
    "    #print(rankings)\n",
    "    return mrr_ret/len(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataloader,model_filename):\n",
    "    checkpoint = torch.load(model_filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return_ranks = {}\n",
    "    number_of_batches = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for X, y in dataloader:\n",
    "        with torch.no_grad():\n",
    "            input_ids = X[0]['input_ids'].squeeze().to(device)\n",
    "            attention_mask = X[0]['attention_mask'].squeeze().to(device)\n",
    "            out = model.forward(input_ids, attention_mask).squeeze().cpu().numpy()\n",
    "            for i, result in enumerate(out):\n",
    "                return_ranks[i] = result   \n",
    "            return_ranks = {key: rank for rank, key in enumerate(sorted(return_ranks, key=return_ranks.get, reverse=True), 1)}\n",
    "        baseline = {}\n",
    "        for i in range(1,101):\n",
    "            baseline[i-1] = i\n",
    "        print('baseline: ',MRR(y, baseline))\n",
    "        print('prediction: ', MRR(y, return_ranks)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQ6Va6c59fP1"
   },
   "outputs": [],
   "source": [
    "def training_loop(epochs, train_bool=True, validate_bool=True, data_dir=\"./data\", saving=False, load=False):\n",
    "    train_set, val_set = dataPreparation()\n",
    "    dataloader_train = createDataloader(train_set, config)\n",
    "    dataloader_val = createDataloader(val_set, config)\n",
    "    \n",
    "    if train_bool or validate_bool:\n",
    "        for epoch in range(epochs):\n",
    "            if load:\n",
    "                current_model_filename = './outputs/model_' + str(epoch+1) + '.pth'\n",
    "                if os.path.exists(current_model_filename):\n",
    "                        checkpoint = torch.load(current_model_filename)\n",
    "                        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                else:\n",
    "                    logger.info(f\"model: {current_model_filename} doesnt exist\")\n",
    "\n",
    "            logger.info(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "            if train_bool:\n",
    "                logger.info(\"Starting training loop...\")\n",
    "                train(model, loss_fn, optimizer, dataloader_train, epoch, saving=saving, tune=False)\n",
    "\n",
    "            if validate_bool:\n",
    "                logger.info(\"Starting validation loop...\")\n",
    "                val(model, loss_fn, dataloader_val, epoch, tune=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word2vec = loadGoogleWord2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, words = averagePollingDocument(model_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def testing_loop(query, data_dir=\"./data\"):\n",
    "    avg_query = averagePollingQuery(query, model_word2vec)\n",
    "    most_similair = getMostSimilair(avg_query, documents)\n",
    "    #print(most_similair)\n",
    "    dataloader = createDataloaderTest(most_similair)\n",
    "    #print(dataloader)\n",
    "    print('query: ', query)\n",
    "    predict(dataloader, './outputs/model_15.pth')\n",
    "    \n",
    "for x in range(10):\n",
    "    testing_loop(createRandomQuery(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C7vVclae-Ze6",
    "outputId": "8e959e24-d0a5-44e9-d177-ade3880bf22c"
   },
   "outputs": [],
   "source": [
    "#training_loop(5, train_bool=False, validate_bool=False, saving=False, load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
