{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwEr46hqDtJK",
        "outputId": "311084f7-f109-4a2f-8087-0c085942a63c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.8/dist-packages (3.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ray in /usr/local/lib/python3.8/dist-packages (2.2.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from ray) (6.0)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.8/dist-packages (from ray) (1.3.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from ray) (1.0.4)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.8/dist-packages (from ray) (1.3.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from ray) (7.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from ray) (3.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from ray) (2.25.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.8/dist-packages (from ray) (4.3.3)\n",
            "Requirement already satisfied: grpcio>=1.32.0 in /usr/local/lib/python3.8/dist-packages (from ray) (1.51.1)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.8/dist-packages (from ray) (22.2.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.8/dist-packages (from ray) (1.21.6)\n",
            "Requirement already satisfied: virtualenv>=20.0.24 in /usr/local/lib/python3.8/dist-packages (from ray) (20.17.1)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.8/dist-packages (from ray) (3.19.6)\n",
            "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.8/dist-packages (from virtualenv>=20.0.24->ray) (0.3.6)\n",
            "Requirement already satisfied: platformdirs<3,>=2.4 in /usr/local/lib/python3.8/dist-packages (from virtualenv>=20.0.24->ray) (2.6.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->ray) (0.19.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->ray) (5.10.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->ray) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->ray) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->ray) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->ray) (2022.12.7)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema->ray) (3.11.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install wget\n",
        "!pip install transformers\n",
        "!pip install ray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "N1mZmWzU18_W"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import wget\n",
        "import gzip\n",
        "import html\n",
        "import tarfile\n",
        "import logging\n",
        "import sys\n",
        "import csv\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, AdamW, BertTokenizer\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from functools import partial\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "mCd6h66j7mKf"
      },
      "outputs": [],
      "source": [
        "class ProductRanker(nn.Module):\n",
        "    def __init__(self, l1=128, l2=32):\n",
        "        super(ProductRanker, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.ranker = nn.Sequential(\n",
        "            nn.Linear(768, l1),\n",
        "            nn.BatchNorm1d(l1, affine=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(l1, l2),\n",
        "            nn.BatchNorm1d(l2, affine=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(l2,1)\n",
        "        )\n",
        "\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "        logits = self.ranker(last_hidden_state_cls)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tA9ILTLDDmkE",
        "outputId": "b5312e23-ee29-4c5c-891f-66d677b616d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting new HTTPS connection (1): huggingface.co:443\n",
            "https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "#Constants\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "config = {'l1': 256, 'l2': 8, 'lr': 0.0008611640946726886, 'batch_size': 4}\n",
        "model = ProductRanker(config[\"l1\"],config[\"l2\"]).to(device)\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], betas=(0.9, 0.999), eps=1e-08, weight_decay=0,\n",
        "                            amsgrad=False)\n",
        "\n",
        "if not os.path.exists(\"./data\"):\n",
        "    os.makedirs(\"./data\")\n",
        "if not os.path.exists(\"./outputs\"):\n",
        "    os.makedirs(\"./outputs\")\n",
        "\n",
        "logging.basicConfig(filename='./outputs/training.log', filemode='w', format='%(asctime)s %(message)s', level=logging.DEBUG, force=True)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
        "logger = logging.getLogger()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "g35D5YJf3Ila"
      },
      "outputs": [],
      "source": [
        "def downloadData(url, path):\n",
        "    return wget.download(url, out=path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "V4-NKG6m3OjZ"
      },
      "outputs": [],
      "source": [
        "def loadJson(filename):\n",
        "    data = []\n",
        "    with gzip.open(filename, \"rt\") as f:\n",
        "        for line in f:\n",
        "            stripped = line.strip()\n",
        "            stripped = stripped.replace(\"\\'\", \"\\\"\")\n",
        "            try:\n",
        "                data.append(json.loads(stripped))\n",
        "            except json.decoder.JSONDecodeError:\n",
        "                continue \n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "CZatCtPg3QRz"
      },
      "outputs": [],
      "source": [
        "def filterJson(data, extrema_dict):\n",
        "    filtered = []\n",
        "    for entry in data:\n",
        "        if not isGoodJsonEntry(entry):\n",
        "            continue\n",
        "        \n",
        "        sales_rank_key = list(entry['salesRank'].keys())[0]\n",
        "        sales_rank_value = entry['salesRank'][sales_rank_key]\n",
        "        min = extrema_dict[sales_rank_key][0]\n",
        "        max = extrema_dict[sales_rank_key][1]\n",
        "\n",
        "        tmp = {}\n",
        "        if min == max:\n",
        "            tmp['salesRank'] = 0.5\n",
        "        else:\n",
        "            tmp['salesRank'] = (sales_rank_value - min) / (max - min)\n",
        "        tmp['description'] = html.unescape(entry['description'])\n",
        "        filtered.append(tmp)\n",
        "\n",
        "    return filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "uFHxm4vW3TU6"
      },
      "outputs": [],
      "source": [
        "def getExtremaDict(data):\n",
        "    extrema_dict = {}\n",
        "    for entry in data:\n",
        "        if not isGoodJsonEntry(entry):\n",
        "            continue\n",
        "\n",
        "        sales_rank_key = list(entry['salesRank'].keys())[0]\n",
        "        sales_rank_value = entry['salesRank'][sales_rank_key]\n",
        "        if sales_rank_key not in extrema_dict:\n",
        "            extrema_dict[sales_rank_key] = [sales_rank_value, sales_rank_value] \n",
        "        else:\n",
        "            if extrema_dict[sales_rank_key][0] > sales_rank_value:\n",
        "                extrema_dict[sales_rank_key][0] = sales_rank_value\n",
        "            elif extrema_dict[sales_rank_key][1] < sales_rank_value:\n",
        "                extrema_dict[sales_rank_key][1] = sales_rank_value\n",
        "\n",
        "    return extrema_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "lUCHiFQx3Vjo"
      },
      "outputs": [],
      "source": [
        "def isGoodJsonEntry(entry):\n",
        "    if 'description' not in entry.keys():\n",
        "        return False\n",
        "    if len(entry['description']) == 0:\n",
        "        return False\n",
        "    if 'salesRank' not in entry.keys():\n",
        "        return False\n",
        "    if len(entry['salesRank']) == 0:\n",
        "        return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "iYBaprFg3YFW"
      },
      "outputs": [],
      "source": [
        "def safeJson(filtered, filename):\n",
        "    with open(filename, 'w') as f:\n",
        "        for object in filtered:\n",
        "            json.dump(object, f)\n",
        "            f.write(os.linesep)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "Qxt6iaSp3Zmv"
      },
      "outputs": [],
      "source": [
        "def getJSON(data_dir=\"./data\"):\n",
        "    filename_meta = os.path.join(data_dir, \"meta_Electronics.json.gz\")\n",
        "    filename_filtered_meta = os.path.join(data_dir, \"filtered_meta_Electronics.tar.gz\")\n",
        "    filename_destination_meta = os.path.join(data_dir, \"filtered_meta_Electronics.json\")\n",
        "\n",
        "    url_meta = \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/meta_Electronics.json.gz\"\n",
        "    \n",
        "    if os.path.exists(filename_destination_meta):\n",
        "        return\n",
        "\n",
        "    if os.path.exists(filename_filtered_meta):\n",
        "        file = tarfile.open(filename_filtered_meta, \"r:gz\")\n",
        "        file.extractall(data_dir)\n",
        "        file.close()\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(filename_meta):\n",
        "        downloadData(url_meta, filename_meta)\n",
        "    \n",
        "    data = loadJson(filename_meta)\n",
        "    extrema_dict = getExtremaDict(data)\n",
        "    filtered = filterJson(data, extrema_dict)\n",
        "    safeJson(filtered, filename_destination_meta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm7U4hna8G7j"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "eMavKxtY8Bv6"
      },
      "outputs": [],
      "source": [
        "def dataPreparation(data_dir=\"./data\"):\n",
        "    path = os.path.join(data_dir, \"filtered_meta_Electronics.json\")\n",
        "    df = pd.read_json(path, lines=True).sample(n = (225 + 5625))\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    tokenized_set = []\n",
        "    for index, row in df.iterrows():\n",
        "        tokenized_set.append([tokenizer.encode_plus(row['description'], truncation = True, return_tensors=\"pt\",\n",
        "                                                    max_length=512, pad_to_max_length=True), row['salesRank']])\n",
        "    return torch.utils.data.random_split(tokenized_set, [150, 75, 5625])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "_-y6YFzn8SWW"
      },
      "outputs": [],
      "source": [
        "def createDataloader(dataset, config, shuffle=True, test=False):\n",
        "    labeled_set = []\n",
        "    if test:\n",
        "        for i, doc1 in enumerate(dataset):\n",
        "            labeled_set.append([[doc1[0]], doc1[1]])\n",
        "    else:\n",
        "        for i, doc1 in enumerate(dataset):\n",
        "            for j, doc2 in enumerate(dataset):\n",
        "                if i != j:\n",
        "                    label = 0.0\n",
        "                    if doc1[1] > doc2[1]:\n",
        "                        label = 1.0\n",
        "                    if doc1[1] == doc2[1]:\n",
        "                        label = 0.5\n",
        "\n",
        "                    labeled_set.append([[doc1[0], doc2[0]], label])\n",
        "    return torch.utils.data.DataLoader(labeled_set, batch_size=int(config[\"batch_size\"]), num_workers=2, shuffle=shuffle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "f7k5M2SU8Zw8"
      },
      "outputs": [],
      "source": [
        "def train(model, loss_fn, optimizer, dataloader, epoch, saving=False, tune=False):\n",
        "    running_loss = 0.0\n",
        "    epoch_steps = 0\n",
        "\n",
        "    train_loss = 0.0\n",
        "    acc_tp_tn = 0\n",
        "    acc_total = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for X,y in dataloader:\n",
        "        input_ids_1 = X[0]['input_ids'].squeeze().to(device)\n",
        "        attention_mask_1 = X[0]['attention_mask'].squeeze().to(device)\n",
        "        input_ids_2 = X[1]['input_ids'].squeeze().to(device)\n",
        "        attention_mask_2 = X[1]['attention_mask'].squeeze().to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out1 = model.forward(input_ids_1, attention_mask_1)\n",
        "        out2 = model.forward(input_ids_2, attention_mask_2)\n",
        "        diff = (out1 - out2).squeeze()\n",
        "        diff = torch.sigmoid(diff).to(device)\n",
        "        loss = loss_fn(diff, y.float().to(device))\n",
        "        \n",
        "        for i in range(len(diff)):\n",
        "            if (diff[i] >= 0.5 and y[i] == 1) or (diff[i] < 0.5 and y[i] == 0):\n",
        "                acc_tp_tn += 1\n",
        "            acc_total +=1\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "    logger.info(\"[%d] train loss: %.10f\" % (epoch + 1, train_loss / len(dataloader)))\n",
        "    logging.info(\"[%d] train accuracy: %.10f\" % (epoch + 1, acc_tp_tn / acc_total))\n",
        "\n",
        "    if saving:\n",
        "        path = './outputs/model_' + str(epoch+1) + '.pth'\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "        }, path)\n",
        "        logger.info(\"Model saved..\")\n",
        "\n",
        "    if tune:\n",
        "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
        "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "            torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
        "\n",
        "    logger.info(\"Training finished!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "V4-vkMnZ8cq7"
      },
      "outputs": [],
      "source": [
        "def val(model, loss_fn, dataloader, epoch, tune=False):\n",
        "        val_loss = 0.0\n",
        "        acc_tp_tn = 0\n",
        "        acc_total = 0\n",
        "        \n",
        "        model.eval()\n",
        "        \n",
        "        for X, y in dataloader:\n",
        "            with torch.no_grad():\n",
        "                input_ids_1 = X[0]['input_ids'].squeeze().to(device)\n",
        "                attention_mask_1 = X[0]['attention_mask'].squeeze().to(device)\n",
        "                input_ids_2 = X[1]['input_ids'].squeeze().to(device)\n",
        "                attention_mask_2 = X[1]['attention_mask'].squeeze().to(device)\n",
        "                out1 = model.forward(input_ids_1, attention_mask_1)\n",
        "                out2 = model.forward(input_ids_2, attention_mask_2)\n",
        "                diff = (out1 - out2).squeeze()\n",
        "                diff = torch.sigmoid(diff)\n",
        "                loss = loss_fn(diff, y.float().to(device))\n",
        "                val_loss += loss.cpu().numpy()\n",
        "                \n",
        "                for i in range(len(diff)):\n",
        "                    if (diff[i] >= 0.5 and y[i] == 1) or (diff[i] < 0.5 and y[i] == 0):\n",
        "                        acc_tp_tn += 1\n",
        "                    acc_total +=1\n",
        "\n",
        "        if(tune):\n",
        "            tune.report(loss=val_loss)\n",
        "\n",
        "        logger.info(\"[%d] val loss: %.10f\" % (epoch+1, val_loss/len(dataloader)))\n",
        "        logging.info(\"[%d] val acc: %.10f\" % (epoch + 1, acc_tp_tn/acc_total))\n",
        "        return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "vgV_KMYRllIi"
      },
      "outputs": [],
      "source": [
        "def test(model, dataloader, epoch):\n",
        "\n",
        "    mse_ = 0.0\n",
        "    model.eval()\n",
        "    \n",
        "    for X, y in dataloader:\n",
        "        with torch.no_grad():\n",
        "            input_ids = X[0]['input_ids'].squeeze().to(device)\n",
        "            attention_mask = X[0]['attention_mask'].squeeze().to(device)\n",
        "            out = model.forward(input_ids, attention_mask).squeeze().cpu().numpy()\n",
        "            mse_ += mean_squared_error(y, out)\n",
        "    \n",
        "    logger.info(\"[%d] val loss: %.10f\" % (epoch+1, mse_/len(dataloader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "PEGxtzDEPLDQ"
      },
      "outputs": [],
      "source": [
        "def train_tune(config, data_dir=\"./data\"):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = ProductRanker(config[\"l1\"],config[\"l2\"]).to(device)\n",
        "    loss_fn = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], betas=(0.9, 0.999), eps=1e-08, weight_decay=0,\n",
        "                                amsgrad=False)\n",
        "    \n",
        "    train_set, val_set = dataPreparation(data_dir)\n",
        "    dataloader_train = createDataloader(train_set, config)\n",
        "    dataloader_val = createDataloader(val_set, config)\n",
        "\n",
        "    train(model, optimizer, dataloader_train, 0, tune=True)\n",
        "    val(model, loss_fn, dataloader_val, 0, tune=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xLrYjbv9QJx"
      },
      "source": [
        "Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "3mrEAanp9OFO"
      },
      "outputs": [],
      "source": [
        "def tune_hyperparameters():\n",
        "    data_dir = os.path.abspath(\"./data\")\n",
        "    config = {\n",
        "        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
        "        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
        "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
        "        \"batch_size\": tune.choice([2, 4, 8, 16])\n",
        "    }\n",
        "    scheduler = ASHAScheduler(\n",
        "        metric=\"loss\",\n",
        "        mode=\"min\",\n",
        "        max_t=4,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2)\n",
        "    reporter = CLIReporter(\n",
        "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
        "        metric_columns=[\"loss\", \"training_iteration\"])\n",
        "    result = tune.run(\n",
        "        partial(train_tune, data_dir=data_dir),\n",
        "        resources_per_trial={\"cpu\": 12, \"gpu\": 1},\n",
        "        config=config,\n",
        "        num_samples=10,\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter)\n",
        "\n",
        "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
        "    print(\"Best trial config: {}\".format(best_trial.config))\n",
        "    print(\"Best trial final validation loss: {}\".format(\n",
        "        best_trial.last_result[\"loss\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "YStntQVRDUGM"
      },
      "outputs": [],
      "source": [
        "def tune_lr():\n",
        "    data_dir = os.path.abspath(\"./data\")\n",
        "    config = {\n",
        "        'l1': 256,\n",
        "        'l2': 8,\n",
        "        'lr': tune.loguniform(1e-4, 1e-1),\n",
        "        'batch_size': 4}\n",
        "    scheduler = ASHAScheduler(\n",
        "        metric=\"loss\",\n",
        "        mode=\"min\",\n",
        "        max_t=5,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2)\n",
        "    reporter = CLIReporter(\n",
        "        metric_columns=[\"loss\", \"training_iteration\"])\n",
        "    result = tune.run(\n",
        "        partial(train_tune, data_dir=data_dir),\n",
        "        resources_per_trial={\"cpu\": 12, \"gpu\": 1},\n",
        "        local_dir=\"./ray_results\",\n",
        "        config=config,\n",
        "        num_samples=5,\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter)\n",
        "\n",
        "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
        "    print(\"Best trial config: {}\".format(best_trial.config))\n",
        "    print(\"Best trial final validation loss: {}\".format(\n",
        "        best_trial.last_result[\"loss\"]))\n",
        "    best_checkpoint = result.get_best_checkpoint(best_trial, metric=\"loss\", mode=\"min\", )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "NmPbsO6w9c7T"
      },
      "outputs": [],
      "source": [
        "def train_model(epochs, config, from_checkpoint=False, path=None):\n",
        "    data_dir = os.path.abspath(\"./data\")\n",
        "    train(epochs, config, data_dir, from_checkpoint, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "rQ6Va6c59fP1"
      },
      "outputs": [],
      "source": [
        "def main(epochs, train_bool=True, validate_bool=True, test_bool=True, data_dir=\"./data\", saving=False, load=False):\n",
        "    # final config = {'l1': 128, 'l2': 32, 'lr': 0.0005, 'batch_size': 100}\n",
        "    getJSON()\n",
        "    train_set, val_set, test_set = dataPreparation()\n",
        "    dataloader_train = createDataloader(train_set, config)\n",
        "    dataloader_val = createDataloader(val_set, config)\n",
        "    dataloader_test = createDataloader(test_set, config, test=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        if load:\n",
        "            current_model_filename = './outputs/model_' + str(epoch+1) + '.pth'\n",
        "            if os.path.exists(current_model_filename):\n",
        "                    checkpoint = torch.load(current_model_filename)\n",
        "                    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "                    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            else:\n",
        "                logger.info(f\"model: {current_model_filename} doesnt exist\")\n",
        "        \n",
        "        logger.info(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
        "        if train_bool:\n",
        "            logger.info(\"Starting training loop...\")\n",
        "            train(model, loss_fn, optimizer, dataloader_train, epoch, saving=saving, tune=False)\n",
        "        \n",
        "        if validate_bool:\n",
        "            logger.info(\"Starting validation loop...\")\n",
        "            val(model, loss_fn, dataloader_val, epoch, tune=False)\n",
        "\n",
        "        if test_bool:\n",
        "            logger.info(\"Starting testing loop...\")\n",
        "            test(model, dataloader_test, epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7vVclae-Ze6",
        "outputId": "8e959e24-d0a5-44e9-d177-ade3880bf22c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting new HTTPS connection (1): huggingface.co:443\n",
            "https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/vocab.txt HTTP/1.1\" 200 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Starting testing loop...\n",
            "[1] val loss: 0.2997781106\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Starting testing loop...\n",
            "[2] val loss: 0.2998836899\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Starting testing loop...\n",
            "[3] val loss: 0.2989454469\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Starting testing loop...\n",
            "[4] val loss: 0.3012032502\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Starting testing loop...\n",
            "[5] val loss: 0.3003966274\n"
          ]
        }
      ],
      "source": [
        "main(5, train_bool=False, validate_bool=False, saving=False, load=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "GBxiaLnjXIfV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}