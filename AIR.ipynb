{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MwEr46hqDtJK",
    "outputId": "311084f7-f109-4a2f-8087-0c085942a63c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wget in /home/niklas/.local/lib/python3.10/site-packages (3.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/niklas/.local/lib/python3.10/site-packages (4.25.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/niklas/.local/lib/python3.10/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/niklas/.local/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/niklas/.local/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/niklas/.local/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/niklas/.local/lib/python3.10/site-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/niklas/.local/lib/python3.10/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/niklas/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ray in /home/niklas/.local/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: packaging in /home/niklas/.local/lib/python3.10/site-packages (from ray) (21.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /home/niklas/.local/lib/python3.10/site-packages (from ray) (1.23.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray) (3.8.0)\n",
      "Requirement already satisfied: attrs in /home/niklas/.local/lib/python3.10/site-packages (from ray) (22.1.0)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from ray) (5.4.1)\n",
      "Requirement already satisfied: frozenlist in /home/niklas/.local/lib/python3.10/site-packages (from ray) (1.3.3)\n",
      "Requirement already satisfied: virtualenv>=20.0.24 in /usr/local/lib/python3.10/dist-packages (from ray) (20.16.7)\n",
      "Requirement already satisfied: jsonschema in /home/niklas/.local/lib/python3.10/site-packages (from ray) (4.17.0)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /home/niklas/.local/lib/python3.10/site-packages (from ray) (4.21.12)\n",
      "Requirement already satisfied: aiosignal in /home/niklas/.local/lib/python3.10/site-packages (from ray) (1.3.1)\n",
      "Requirement already satisfied: grpcio>=1.42.0 in /home/niklas/.local/lib/python3.10/site-packages (from ray) (1.51.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/niklas/.local/lib/python3.10/site-packages (from ray) (1.0.4)\n",
      "Requirement already satisfied: click>=7.0 in /usr/lib/python3/dist-packages (from ray) (8.0.3)\n",
      "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.0.24->ray) (0.3.6)\n",
      "Requirement already satisfied: platformdirs<3,>=2.4 in /home/niklas/.local/lib/python3.10/site-packages (from virtualenv>=20.0.24->ray) (2.5.4)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/niklas/.local/lib/python3.10/site-packages (from jsonschema->ray) (0.19.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging->ray) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (2022.12.7)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/niklas/.local/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/niklas/.local/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/niklas/.local/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/niklas/.local/lib/python3.10/site-packages (from scikit-learn) (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/niklas/.local/lib/python3.10/site-packages (from scikit-learn) (1.23.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install wget\n",
    "!pip install transformers\n",
    "!pip install ray\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "N1mZmWzU18_W"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import wget\n",
    "import gzip\n",
    "import html\n",
    "import tarfile\n",
    "import logging\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, AdamW, BertTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mCd6h66j7mKf"
   },
   "outputs": [],
   "source": [
    "class ProductRanker(nn.Module):\n",
    "    def __init__(self, l1=128, l2=32):\n",
    "        super(ProductRanker, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.ranker = nn.Sequential(\n",
    "            nn.Linear(768, l1),\n",
    "            nn.BatchNorm1d(l1, affine=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(l1, l2),\n",
    "            nn.BatchNorm1d(l2, affine=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(l2,1)\n",
    "        )\n",
    "\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        logits = self.ranker(last_hidden_state_cls)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tA9ILTLDDmkE",
    "outputId": "b5312e23-ee29-4c5c-891f-66d677b616d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niklas/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:88: UserWarning: HIP initialization: Unexpected error from hipGetDeviceCount(). Did you run some cuda functions before calling NumHipDevices() that might have already set an error? Error 101: hipErrorInvalidDevice (Triggered internally at ../c10/hip/HIPFunctions.cpp:110.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#Constants\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config = {'l1': 128, 'l2': 32, 'lr': 0.0005, 'batch_size': 100}\n",
    "model = ProductRanker(config[\"l1\"],config[\"l2\"]).to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], betas=(0.9, 0.999), eps=1e-08, weight_decay=0,\n",
    "                            amsgrad=False)\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "if not os.path.exists(\"./data\"):\n",
    "    os.makedirs(\"./data\")\n",
    "if not os.path.exists(\"./outputs\"):\n",
    "    os.makedirs(\"./outputs\")\n",
    "\n",
    "logging.basicConfig(filename='./outputs/training.log', filemode='w', format='%(asctime)s %(message)s', level=logging.DEBUG, force=True)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "g35D5YJf3Ila"
   },
   "outputs": [],
   "source": [
    "def downloadData(url, path):\n",
    "    return wget.download(url, out=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "V4-NKG6m3OjZ"
   },
   "outputs": [],
   "source": [
    "def loadJson(filename):\n",
    "    data = []\n",
    "    with gzip.open(filename, \"rt\") as f:\n",
    "        for line in f:\n",
    "            stripped = line.strip()\n",
    "            stripped = stripped.replace(\"\\'\", \"\\\"\")\n",
    "            try:\n",
    "                data.append(json.loads(stripped))\n",
    "            except json.decoder.JSONDecodeError:\n",
    "                continue \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CZatCtPg3QRz"
   },
   "outputs": [],
   "source": [
    "def filterJson(data, extrema_dict):\n",
    "    filtered = []\n",
    "    for entry in data:\n",
    "        if not isGoodJsonEntry(entry):\n",
    "            continue\n",
    "        \n",
    "        sales_rank_key = list(entry['salesRank'].keys())[0]\n",
    "        sales_rank_value = entry['salesRank'][sales_rank_key]\n",
    "        min = extrema_dict[sales_rank_key][0]\n",
    "        max = extrema_dict[sales_rank_key][1]\n",
    "\n",
    "        tmp = {}\n",
    "        if min == max:\n",
    "            tmp['salesRank'] = 0.5\n",
    "        else:\n",
    "            tmp['salesRank'] = (sales_rank_value - min) / (max - min)\n",
    "        tmp['description'] = html.unescape(entry['description'])\n",
    "        filtered.append(tmp)\n",
    "\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uFHxm4vW3TU6"
   },
   "outputs": [],
   "source": [
    "def getExtremaDict(data):\n",
    "    extrema_dict = {}\n",
    "    for entry in data:\n",
    "        if not isGoodJsonEntry(entry):\n",
    "            continue\n",
    "\n",
    "        sales_rank_key = list(entry['salesRank'].keys())[0]\n",
    "        sales_rank_value = entry['salesRank'][sales_rank_key]\n",
    "        if sales_rank_key not in extrema_dict:\n",
    "            extrema_dict[sales_rank_key] = [sales_rank_value, sales_rank_value] \n",
    "        else:\n",
    "            if extrema_dict[sales_rank_key][0] > sales_rank_value:\n",
    "                extrema_dict[sales_rank_key][0] = sales_rank_value\n",
    "            elif extrema_dict[sales_rank_key][1] < sales_rank_value:\n",
    "                extrema_dict[sales_rank_key][1] = sales_rank_value\n",
    "\n",
    "    return extrema_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lUCHiFQx3Vjo"
   },
   "outputs": [],
   "source": [
    "def isGoodJsonEntry(entry):\n",
    "    if 'description' not in entry.keys():\n",
    "        return False\n",
    "    if len(entry['description']) == 0:\n",
    "        return False\n",
    "    if 'salesRank' not in entry.keys():\n",
    "        return False\n",
    "    if len(entry['salesRank']) == 0:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "iYBaprFg3YFW"
   },
   "outputs": [],
   "source": [
    "def safeJson(filtered, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for object in filtered:\n",
    "            json.dump(object, f)\n",
    "            f.write(os.linesep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Qxt6iaSp3Zmv"
   },
   "outputs": [],
   "source": [
    "def getJSON(data_dir=\"./data\"):\n",
    "    filename_meta = os.path.join(data_dir, \"meta_Electronics.json.gz\")\n",
    "    filename_filtered_meta = os.path.join(data_dir, \"filtered_meta_Electronics.tar.gz\")\n",
    "    filename_destination_meta_train = os.path.join(data_dir, \"filtered_meta_Electronics_train.json\")\n",
    "    filename_destination_meta_test = os.path.join(data_dir, \"filtered_meta_Electronics_test.json\")\n",
    "\n",
    "    url_meta = \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/meta_Electronics.json.gz\"\n",
    "    \n",
    "    if os.path.exists(filename_destination_meta_train) and os.path.exists(filename_destination_meta_test):\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(filename_meta):\n",
    "        downloadData(url_meta, filename_meta)\n",
    "    \n",
    "    data = loadJson(filename_meta)\n",
    "    extrema_dict = getExtremaDict(data)\n",
    "    filtered = filterJson(data, extrema_dict)\n",
    "    train_set, test_set = torch.utils.data.random_split(filtered, [len(filtered)-5625, 5625])\n",
    "    safeJson(train_set, filename_destination_meta_train)\n",
    "    safeJson(test_set, filename_destination_meta_test)\n",
    "    \n",
    "getJSON()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGoogleWord2Vec():\n",
    "    return api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "model_word2vec = loadGoogleWord2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_word2vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [35], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m         documents[index] \u001b[38;5;241m=\u001b[39m tmp\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;28mlen\u001b[39m(description))\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m documents\n\u001b[0;32m---> 20\u001b[0m documents \u001b[38;5;241m=\u001b[39m averagePollingDocument(\u001b[43mmodel_word2vec\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_word2vec' is not defined"
     ]
    }
   ],
   "source": [
    "def averagePollingDocument(model_word2vec, data_dir=\"./data\"):\n",
    "    path = os.path.join(data_dir, \"filtered_meta_Electronics_test.json\")\n",
    "    df = pd.read_json(path, lines=True) \n",
    "    \n",
    "    len_vec = len(model_word2vec[\"hello\"])\n",
    "    \n",
    "    documents = dict()\n",
    "    for index, row in df.iterrows():\n",
    "        tmp = torch.zeros(len_vec)\n",
    "        description = utils.simple_preprocess(row['description'])\n",
    "        for word in description:\n",
    "            try:\n",
    "                tmp = tmp.add(torch.tensor(model_word2vec[word]))\n",
    "            except KeyError:\n",
    "                continue\n",
    "        documents[index] = tmp.div(len(description))\n",
    "        \n",
    "    return documents\n",
    "\n",
    "documents = averagePollingDocument(model_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor(0.4788), 8750), (tensor(0.4092), 15011), (tensor(0.4070), 17988)]\n"
     ]
    }
   ],
   "source": [
    "def averagePollingQuery(query, model_word2vec):\n",
    "    query = utils.simple_preprocess(query)\n",
    "    len_vec = len(model_word2vec[\"hello\"])\n",
    "    \n",
    "    avg_query = torch.zeros(len_vec)\n",
    "    for word in query:\n",
    "        try:\n",
    "            avg_query += avg_query.add(torch.tensor(model_word2vec[word]))\n",
    "        except KeyError:\n",
    "            continue\n",
    "    avg_query = avg_query / len(query)\n",
    "    return avg_query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMostSimilair(avg_query, documents, x=10):\n",
    "    avg_query = averagePollingQuery(\"test test HELLO\", model_word2vec)\n",
    "    cosine_sim = [(cos(avg_query, value), indx) for indx, value in documents.items()]\n",
    "    cosine_sim.sort(key=lambda y: -y[0].item())\n",
    "    most_similair = cosine_sim[:x]\n",
    "    return most_similair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "eMavKxtY8Bv6"
   },
   "outputs": [],
   "source": [
    "def dataPreparation(data_dir=\"./data\"):\n",
    "    path = os.path.join(data_dir, \"filtered_meta_Electronics_train.json\")\n",
    "    df = pd.read_json(path, lines=True).sample(n = (225))\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    tokenized_set = []\n",
    "    for index, row in df.iterrows():\n",
    "        tokenized_set.append([tokenizer.encode_plus(row['description'], truncation = True, return_tensors=\"pt\",\n",
    "                                                    max_length=512, pad_to_max_length=True), row['salesRank']])\n",
    "    return torch.utils.data.random_split(tokenized_set, [150, 75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_-y6YFzn8SWW"
   },
   "outputs": [],
   "source": [
    "def createDataloader(dataset, config, shuffle=True, test=False):\n",
    "    labeled_set = []\n",
    "    if test:\n",
    "        for i, doc1 in enumerate(dataset):\n",
    "            labeled_set.append([[doc1[0]], doc1[1]])\n",
    "    else:\n",
    "        for i, doc1 in enumerate(dataset):\n",
    "            for j, doc2 in enumerate(dataset):\n",
    "                if i != j:\n",
    "                    label = 0.0\n",
    "                    if doc1[1] > doc2[1]:\n",
    "                        label = 1.0\n",
    "                    if doc1[1] == doc2[1]:\n",
    "                        label = 0.5\n",
    "\n",
    "                    labeled_set.append([[doc1[0], doc2[0]], label])\n",
    "    return torch.utils.data.DataLoader(labeled_set, batch_size=int(config[\"batch_size\"]), num_workers=2, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "f7k5M2SU8Zw8"
   },
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, dataloader, epoch, saving=False, tune=False):\n",
    "    running_loss = 0.0\n",
    "    epoch_steps = 0\n",
    "\n",
    "    train_loss = 0.0\n",
    "    acc_tp_tn = 0\n",
    "    acc_total = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for X,y in dataloader:\n",
    "        input_ids_1 = X[0]['input_ids'].squeeze().to(device)\n",
    "        attention_mask_1 = X[0]['attention_mask'].squeeze().to(device)\n",
    "        input_ids_2 = X[1]['input_ids'].squeeze().to(device)\n",
    "        attention_mask_2 = X[1]['attention_mask'].squeeze().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out1 = model.forward(input_ids_1, attention_mask_1)\n",
    "        out2 = model.forward(input_ids_2, attention_mask_2)\n",
    "        diff = (out1 - out2).squeeze()\n",
    "        diff = torch.sigmoid(diff).to(device)\n",
    "        loss = loss_fn(diff, y.float().to(device))\n",
    "        \n",
    "        for i in range(len(diff)):\n",
    "            if (diff[i] >= 0.5 and y[i] == 1) or (diff[i] < 0.5 and y[i] == 0):\n",
    "                acc_tp_tn += 1\n",
    "            acc_total +=1\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    logger.info(\"[%d] train loss: %.10f\" % (epoch + 1, train_loss / len(dataloader)))\n",
    "    logging.info(\"[%d] train accuracy: %.10f\" % (epoch + 1, acc_tp_tn / acc_total))\n",
    "\n",
    "    if saving:\n",
    "        path = './outputs/model_' + str(epoch+1) + '.pth'\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }, path)\n",
    "        logger.info(\"Model saved..\")\n",
    "\n",
    "    if tune:\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "    logger.info(\"Training finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "V4-vkMnZ8cq7"
   },
   "outputs": [],
   "source": [
    "def val(model, loss_fn, dataloader, epoch, tune=False):\n",
    "        val_loss = 0.0\n",
    "        acc_tp_tn = 0\n",
    "        acc_total = 0\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        for X, y in dataloader:\n",
    "            with torch.no_grad():\n",
    "                input_ids_1 = X[0]['input_ids'].squeeze().to(device)\n",
    "                attention_mask_1 = X[0]['attention_mask'].squeeze().to(device)\n",
    "                input_ids_2 = X[1]['input_ids'].squeeze().to(device)\n",
    "                attention_mask_2 = X[1]['attention_mask'].squeeze().to(device)\n",
    "                out1 = model.forward(input_ids_1, attention_mask_1)\n",
    "                out2 = model.forward(input_ids_2, attention_mask_2)\n",
    "                diff = (out1 - out2).squeeze()\n",
    "                diff = torch.sigmoid(diff)\n",
    "                loss = loss_fn(diff, y.float().to(device))\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                \n",
    "                for i in range(len(diff)):\n",
    "                    if (diff[i] >= 0.5 and y[i] == 1) or (diff[i] < 0.5 and y[i] == 0):\n",
    "                        acc_tp_tn += 1\n",
    "                    acc_total +=1\n",
    "\n",
    "        if(tune):\n",
    "            tune.report(loss=val_loss)\n",
    "\n",
    "        logger.info(\"[%d] val loss: %.10f\" % (epoch+1, val_loss/len(dataloader)))\n",
    "        logging.info(\"[%d] val acc: %.10f\" % (epoch + 1, acc_tp_tn/acc_total))\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "PEGxtzDEPLDQ"
   },
   "outputs": [],
   "source": [
    "def train_tune(config, data_dir=\"./data\"):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = ProductRanker(config[\"l1\"],config[\"l2\"]).to(device)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], betas=(0.9, 0.999), eps=1e-08, weight_decay=0,\n",
    "                                amsgrad=False)\n",
    "    \n",
    "    train_set, val_set = dataPreparation(data_dir)\n",
    "    dataloader_train = createDataloader(train_set, config)\n",
    "    dataloader_val = createDataloader(val_set, config)\n",
    "\n",
    "    train(model, optimizer, dataloader_train, 0, tune=True)\n",
    "    val(model, loss_fn, dataloader_val, 0, tune=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3mrEAanp9OFO"
   },
   "outputs": [],
   "source": [
    "def tune_hyperparameters():\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    config = {\n",
    "        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([2, 4, 8, 16])\n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=4,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    reporter = CLIReporter(\n",
    "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        partial(train_tune, data_dir=data_dir),\n",
    "        resources_per_trial={\"cpu\": 12, \"gpu\": 1},\n",
    "        config=config,\n",
    "        num_samples=10,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "YStntQVRDUGM"
   },
   "outputs": [],
   "source": [
    "def tune_lr():\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    config = {\n",
    "        'l1': 256,\n",
    "        'l2': 8,\n",
    "        'lr': tune.loguniform(1e-4, 1e-1),\n",
    "        'batch_size': 4}\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=5,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    reporter = CLIReporter(\n",
    "        metric_columns=[\"loss\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        partial(train_tune, data_dir=data_dir),\n",
    "        resources_per_trial={\"cpu\": 12, \"gpu\": 1},\n",
    "        local_dir=\"./ray_results\",\n",
    "        config=config,\n",
    "        num_samples=5,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    best_checkpoint = result.get_best_checkpoint(best_trial, metric=\"loss\", mode=\"min\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "NmPbsO6w9c7T"
   },
   "outputs": [],
   "source": [
    "def train_model(epochs, config, from_checkpoint=False, path=None):\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    train(epochs, config, data_dir, from_checkpoint, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataloader,model_filename):\n",
    "    checkpoint = torch.load(model_filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return_ranks = {}\n",
    "    number_of_batches = 0\n",
    "    for X, y in dataloader:\n",
    "        with torch.no_grad():\n",
    "            input_ids = X[0]['input_ids'].squeeze().to(device)\n",
    "            attention_mask = X[0]['attention_mask'].squeeze().to(device)\n",
    "            out = model.forward(input_ids, attention_mask).squeeze().cpu().numpy()\n",
    "            for i, result in enumerate(out):\n",
    "                return_ranks[i+number_of_batches*100] = result       \n",
    "        number_of_batches += 1       \n",
    "\n",
    "    return {key: rank for rank, key in enumerate(sorted(return_ranks, key=return_ranks.get, reverse=True), 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "rQ6Va6c59fP1"
   },
   "outputs": [],
   "source": [
    "def training_loop(epochs, train_bool=True, validate_bool=True, data_dir=\"./data\", saving=False, load=False):\n",
    "    getJSON()\n",
    "    train_set, val_set = dataPreparation()\n",
    "    dataloader_train = createDataloader(train_set, config)\n",
    "    dataloader_val = createDataloader(val_set, config)\n",
    "    \n",
    "    if train_bool or validate_bool:\n",
    "        for epoch in range(epochs):\n",
    "            if load:\n",
    "                current_model_filename = './outputs/model_' + str(epoch+1) + '.pth'\n",
    "                if os.path.exists(current_model_filename):\n",
    "                        checkpoint = torch.load(current_model_filename)\n",
    "                        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                else:\n",
    "                    logger.info(f\"model: {current_model_filename} doesnt exist\")\n",
    "\n",
    "            logger.info(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "            if train_bool:\n",
    "                logger.info(\"Starting training loop...\")\n",
    "                train(model, loss_fn, optimizer, dataloader_train, epoch, saving=saving, tune=False)\n",
    "\n",
    "            if validate_bool:\n",
    "                logger.info(\"Starting validation loop...\")\n",
    "                val(model, loss_fn, dataloader_val, epoch, tune=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C7vVclae-Ze6",
    "outputId": "8e959e24-d0a5-44e9-d177-ade3880bf22c"
   },
   "outputs": [],
   "source": [
    "#training_loop(5, train_bool=False, validate_bool=False, saving=False, load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new HTTPS connection (1): huggingface.co:443\n",
      "https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/vocab.txt HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christian/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{66: 1, 87: 2, 27: 3, 0: 4, 46: 5, 99: 6, 18: 7, 68: 8, 69: 9, 53: 10, 3: 11, 5: 12, 43: 13, 84: 14, 11: 15, 31: 16, 20: 17, 81: 18, 45: 19, 17: 20, 59: 21, 50: 22, 52: 23, 72: 24, 6: 25, 38: 26, 60: 27, 78: 28, 24: 29, 35: 30, 64: 31, 33: 32, 83: 33, 26: 34, 73: 35, 57: 36, 36: 37, 41: 38, 37: 39, 8: 40, 19: 41, 2: 42, 40: 43, 16: 44, 56: 45, 75: 46, 10: 47, 39: 48, 7: 49, 49: 50, 67: 51, 32: 52, 4: 53, 71: 54, 97: 55, 54: 56, 48: 57, 42: 58, 80: 59, 79: 60, 34: 61, 58: 62, 51: 63, 76: 64, 89: 65, 47: 66, 9: 67, 28: 68, 1: 69, 25: 70, 29: 71, 94: 72, 44: 73, 62: 74, 22: 75, 74: 76, 95: 77, 86: 78, 61: 79, 70: 80, 77: 81, 96: 82, 85: 83, 14: 84, 12: 85, 15: 86, 98: 87, 92: 88, 93: 89, 13: 90, 88: 91, 30: 92, 65: 93, 90: 94, 55: 95, 63: 96, 23: 97, 21: 98, 82: 99, 91: 100}\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join('./data', \"filtered_meta_Electronics_test.json\")\n",
    "df = pd.read_json(path, lines=True).sample(n = (100))\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "tokenized_set = []\n",
    "for index, row in df.iterrows():\n",
    "    tokenized_set.append([tokenizer.encode_plus(row['description'], truncation = True, return_tensors=\"pt\",\n",
    "                                                max_length=512, pad_to_max_length=True), row['salesRank']])\n",
    "\n",
    "dataloader = createDataloader(tokenized_set, config, test=True)\n",
    "print(predict(dataloader, './outputs/model_15.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
